{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from boruta import BorutaPy\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, RFE, mutual_info_classif, SelectKBest, f_classif, chi2\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, ElasticNet\n",
    "from feature_selection_package.evaluation import performance_score, single_evaluation, full_evaluation\n",
    "from feature_selection_package.feature_selectors import CorrelationSelector, MutualInformationSelector, RandomForestSelector, EnsembleSelector\n",
    "from boruta import BorutaPy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data artificial\n",
    "artificial_train_data = pd.read_csv('data/data_sms/artificial_train.data',header=None,sep=' ').dropna(axis=1)\n",
    "artificial_train_labels = pd.read_csv('data/data_sms/artificial_train.labels',header=None,sep=' ').dropna(axis=1)\n",
    "artificial_valid_data = pd.read_csv('data/data_sms/artificial_valid.data',header=None,sep=' ').dropna(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    1000\n",
       " 1    1000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artificial_train_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = artificial_train_data.loc[:np.round(artificial_train_data.shape[0]*0.8)]\n",
    "train_labels = artificial_train_labels.loc[:np.round(artificial_train_labels.shape[0]*0.8)].replace(-1,0).values.ravel()\n",
    "valid_data = artificial_train_data.loc[np.round(artificial_train_data.shape[0]*0.8):]\n",
    "valid_labels = artificial_train_labels.loc[np.round(artificial_train_labels.shape[0]*0.8):].replace(-1,0).values.ravel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if train and valid data is balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of observations in each class in train set: (array([0, 1], dtype=int64), array([793, 808], dtype=int64))\n",
      "number of observations in each class in validation set: (array([0, 1], dtype=int64), array([208, 192], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print('number of observations in each class in train set:', np.unique(train_labels, return_counts=True))\n",
    "print('number of observations in each class in validation set:', np.unique(valid_labels, return_counts=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm1 = SVC(kernel='rbf', C=1, random_state=0)\n",
    "svm2 = SVC(kernel='linear', C=1, random_state=0)\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=0)\n",
    "xgboost = XGBClassifier(learning_rate=0.1, n_estimators=100, max_depth=5, random_state=0)\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=0)\n",
    "\n",
    "logreg = LogisticRegression(penalty='l2', C=1, random_state=0, max_iter=1000)\n",
    "\n",
    "classifiers = np.array([svm1, svm2, tree, xgboost, rfc, logreg])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = [10, 20, 30, 50, 75]\n",
    "pca_results = pd.DataFrame()\n",
    "for n in n_features:\n",
    "    selector = [PCA(n_components=n)]\n",
    "    pca_df = full_evaluation(train_data, train_labels, valid_data, valid_labels, selector, classifiers, dataset_type='artificial')\n",
    "    pca_results = pd.concat([pca_results, pca_df])\n",
    "pca_results.to_csv('data/results_artificial/pca.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = [10, 20, 50]\n",
    "rfe_results = pd.DataFrame()\n",
    "for n in n_features:\n",
    "    selector = [RFE(estimator=RandomForestClassifier(n_estimators=100, max_depth=3), n_features_to_select=n, step=1, verbose=1)]\n",
    "    df = full_evaluation(train_data, train_labels, valid_data, valid_labels, selector, classifiers, dataset_type='artificial')\n",
    "    rfe_results = pd.concat([rfe_results, df])\n",
    "rfe_results.to_csv('data/results_artificial/RFE.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeded methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_df = pd.DataFrame(columns=['Selector', 'Classifier', 'Number_of_Features', 'Accuracy', 'Performance_score', 'Supported_Features'])\n",
    "Cs = [0.001, 0.01, 0.05, 0.1, 0.5, 1, 10]\n",
    "for C in Cs:\n",
    "    lasso = LogisticRegression(penalty='l1', C=C, solver='liblinear', random_state=0)\n",
    "    lasso.fit(train_data, train_labels)\n",
    "    n_features = sum(lasso.coef_[0] != 0)\n",
    "    supported_features = (lasso.coef_[0] != 0)\n",
    "    score = balanced_accuracy_score(valid_labels, lasso.predict(valid_data))\n",
    "    perf_score = performance_score(score, n_features, dataset_type='sms')\n",
    "    lasso_df = pd.concat([lasso_df, pd.DataFrame({'Selector': ['Lasso'], 'Classifier': ['Lasso'], 'Number_of_Features': [n_features], 'Accuracy': [score], 'Performance_score': [perf_score], 'Supported_Features': [supported_features]})], ignore_index=True)\n",
    "\n",
    "lasso_df.to_csv('data/results_artificial/lasso.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.001, 0.01, 0.05, 0.1, 0.5, 1, 10]\n",
    "l1_ratio = [0.5, 0.7, 0.9]\n",
    "elastic_df = pd.DataFrame(columns=['Selector', 'Classifier', 'Number_of_Features', 'Accuracy', 'Performance_score', 'alpha', 'l1_ratio', 'Supported_Features'])\n",
    "for ratio in l1_ratio:\n",
    "    for alpha in alphas:\n",
    "        elastic = ElasticNet(alpha=alpha, l1_ratio=ratio, random_state=0, max_iter = 10000)\n",
    "        elastic.fit(train_data, train_labels)\n",
    "        n_features = sum(elastic.coef_!= 0)\n",
    "        supported_features = (elastic.coef_[0] != 0)\n",
    "        y_pred = np.where(elastic.predict(valid_data) > 0.5, 1, 0)\n",
    "        score = balanced_accuracy_score(valid_labels, y_pred)\n",
    "        perf_score = performance_score(score, n_features, dataset_type='sms')\n",
    "        elastic_df = pd.concat([elastic_df, pd.DataFrame({'Selector': ['ElasticNet'], 'Classifier': ['ElasticNet'], 'Number_of_Features': [n_features], 'Accuracy': [score], 'Performance_score': [perf_score], \"alpha\": [alpha], \"l1_ratio\": [ratio], 'Supported_Features': [supported_features]})], ignore_index=True)\n",
    "\n",
    "elastic_df.to_csv('data/results_artificial/elasticNet.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = [10, 20, 30, 50, 75, 100]\n",
    "forest_results = pd.DataFrame()\n",
    "for n in n_features:\n",
    "    selector = [RandomForestSelector(n_features=n)]\n",
    "    df = full_evaluation(train_data, train_labels, valid_data, valid_labels, selector, classifiers, dataset_type='artificial')\n",
    "    forest_results = pd.concat([forest_results, df])\n",
    "forest_results.to_csv('data/results_artificial/forest.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = [10, 20, 30, 50, 75, 100]\n",
    "corr_results = pd.DataFrame()\n",
    "for n in n_features:\n",
    "    selector = [CorrelationSelector(n_features=n)]\n",
    "    df = full_evaluation(train_data, train_labels, valid_data, valid_labels, selector, classifiers, dataset_type='artificial')\n",
    "    corr_results = pd.concat([corr_results, df])\n",
    "corr_results.to_csv('data/results_artificial/corr.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = [10, 20, 30, 50, 75, 100]\n",
    "mutual_results = pd.DataFrame()\n",
    "for n in n_features:\n",
    "    selector = [MutualInformationSelector(n_features=n)]\n",
    "    df = full_evaluation(train_data, train_labels, valid_data, valid_labels, selector, classifiers, dataset_type='artificial')\n",
    "    mutual_results = pd.concat([mutual_results, df])\n",
    "mutual_results.to_csv('data/results_artificial/mutual.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select K - Best"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = [10, 20, 30, 50, 75, 100]\n",
    "anova_results = pd.DataFrame()\n",
    "for n in n_features:\n",
    "    selector = [SelectKBest(f_classif, k=n)]\n",
    "    df = full_evaluation(train_data, train_labels, valid_data, valid_labels, selector, classifiers, dataset_type='artificial')\n",
    "    anova_results = pd.concat([anova_results, df])\n",
    "anova_results.to_csv('data/results_artificial/anova.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = [10, 20, 30, 50, 75, 100]\n",
    "chi2_results = pd.DataFrame()\n",
    "for n in n_features:\n",
    "    selector = [SelectKBest(chi2, k=n)]\n",
    "    df = full_evaluation(train_data, train_labels, valid_data, valid_labels, selector, classifiers, dataset_type='artificial')\n",
    "    chi2_results = pd.concat([chi2_results, df])\n",
    "chi2_results.to_csv('data/results_artificial/chi2.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid + wrapper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boruta algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = [BorutaPy(estimator=RandomForestClassifier(n_estimators=100, max_depth=3), n_estimators='auto', verbose=1, random_state=0)]\n",
    "boruta_results = full_evaluation(train_data, train_labels, valid_data, valid_labels, selector, classifiers, dataset_type='artificial')\n",
    "boruta_results.to_csv('data/results_artificial/boruta.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features_rfs = [100, 200]\n",
    "n_features_rfe = [10, 25, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_results = pd.DataFrame()\n",
    "for n in n_features_rfs:\n",
    "    for m in n_features_rfe:\n",
    "        selector1 = RandomForestSelector(n_features=n)\n",
    "        selector2 = RFE(estimator=RandomForestClassifier(n_estimators=100, max_depth=3), n_features_to_select=m, step=1, verbose=1)\n",
    "        selectors = [[selector1, selector2]]\n",
    "        df = full_evaluation(train_data, train_labels, valid_data, valid_labels, selectors, classifiers, dataset_type='artificial')\n",
    "        stack_results = pd.concat([stack_results, df])\n",
    "        stack_results.to_csv('data/stack.csv', index=False)\n",
    "\n",
    "stack_results.to_csv('data/results_artificial/stack.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = [10, 20, 30, 50, 75, 100]\n",
    "ensemble_results = pd.DataFrame()\n",
    "for n in n_features:\n",
    "    selectors = [RandomForestSelector(n_features=n), SelectKBest(f_classif, k=n), SelectKBest(chi2, k=n), CorrelationSelector(n_features=n), MutualInformationSelector(n_features=n)]\n",
    "    ensemble = [EnsembleSelector(selectors=selectors)]\n",
    "    df = full_evaluation(train_data, train_labels, valid_data, valid_labels, ensemble, classifiers, dataset_type='artificial')\n",
    "    ensemble_results = pd.concat([ensemble_results, df])\n",
    "    \n",
    "ensemble_results.to_csv('data/results_artificial/ensemble.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_dataframe(path):\n",
    "    df = pd.DataFrame()\n",
    "    files = os.listdir(path)\n",
    "    for file in files:\n",
    "        df_temp = pd.read_csv(path + file)\n",
    "        df = pd.concat([df, df_temp])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arti = create_dataframe('data/results_artificial/').drop_duplicates().reset_index(drop=True)\n",
    "best_features_artificial = arti.sort_values('Performance_score', ascending=False)['Supported_Features'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_list = best_features_artificial.replace('\\n', '')\n",
    "string_list = ' '.join(string_list.split()) \n",
    "string_list = string_list.replace('False', 'False,')\n",
    "string_list = string_list.replace('True', 'True,')\n",
    "my_array = np.array(eval(string_list[:-2] + ']'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.where(my_array == True)[0]\n",
    "np.savetxt('data/best_features_artificial.txt', features, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artificial_train_data = artificial_train_data[features]\n",
    "artificial_valid_data = artificial_valid_data[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost = XGBClassifier(learning_rate=0.1, n_estimators=100, max_depth=5, random_state=0)\n",
    "xgboost.fit(artificial_train_data, artificial_train_labels)\n",
    "pred = xgboost.predict_proba(artificial_valid_data)\n",
    "\n",
    "np.savetxt('data/results_artificial/predictions_artificial.txt', pred[:,1], fmt='%1.5f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
