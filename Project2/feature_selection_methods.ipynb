{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from boruta import BorutaPy\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, RFE, mutual_info_classif, SelectKBest, f_classif, chi2\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, ElasticNet\n",
    "from feature_selection_package.evaluation import performance_score, single_evaluation, full_evaluation\n",
    "from feature_selection_package.feature_selectors import CorrelationSelector, MutualInformationSelector, RandomForestSelector, EnsembleSelector\n",
    "from boruta import BorutaPy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_word_counts(df):\n",
    "    vectorizer = CountVectorizer()\n",
    "    word_counts = vectorizer.fit_transform(df['message'])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    counts_df = pd.DataFrame(word_counts.toarray(), columns=feature_names)\n",
    "    result_df = pd.concat([df['label'], counts_df], axis=1)\n",
    "    return result_df\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data artificial\n",
    "artificial_train_data = pd.read_csv('data/artificial_train.data',header=None,sep=' ').dropna(axis=1)\n",
    "artificial_train_labels = pd.read_csv('data/artificial_train.labels',header=None,sep=' ').dropna(axis=1)\n",
    "artificial_valid_data = pd.read_csv('data/artificial_valid.data',header=None,sep=' ').dropna(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data sms\n",
    "sms_train = pd.read_csv('data/sms_train.csv')\n",
    "sms_train_data, sms_train_labels = sms_train.iloc[:, 1], sms_train.iloc[:, 0]\n",
    "\n",
    "sms_test_data = pd.read_csv('data/sms_test.csv')\n",
    "sms_test = sms_test_data.copy()\n",
    "sms_test['label'] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is no use for that dataset\n",
    "# sms = pd.read_csv('data/sms.tsv', header=None, sep='\\t')\n",
    "# sms.columns = ['label', 'message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ = pd.concat([sms_train, sms_test], axis=0).reset_index(drop=True)\n",
    "counted_sms = get_word_counts(summ)\n",
    "# labels are in the first column ;)\n",
    "sms_train_data_final = counted_sms.iloc[:-1000,:]\n",
    "sms_test_data_final = counted_sms.iloc[-1000:,:].reset_index(drop=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm1 = SVC(kernel='rbf', C=1, random_state=0)\n",
    "svm2 = SVC(kernel='linear', C=1, random_state=0)\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=0)\n",
    "xgboost = XGBClassifier(learning_rate=0.1, n_estimators=100, max_depth=5, random_state=0)\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=0)\n",
    "\n",
    "logreg = LogisticRegression(penalty='l2', C=1, random_state=0)\n",
    "\n",
    "classifiers = np.array([svm1, svm2, tree, xgboost, rfc, logreg])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = [10, 20, 50, 80]\n",
    "pca_results = pd.DataFrame()\n",
    "for n in n_features:\n",
    "    selector = [PCA(n_components=n)]\n",
    "    pca_df = full_evaluation(train_data, train_labels, valid_data, valid_labels, selector, classifiers)\n",
    "    pca_results = pd.concat([pca_results, pca_df])\n",
    "pca_results.to_csv('data/pca.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = [100, 1000, 7000]\n",
    "selector = [RFE(estimator=RandomForestClassifier(n_estimators=100, max_depth=3), n_features_to_select=7000, step=1, verbose=0)]\n",
    "rfe_results = full_evaluation(train_data, train_labels, valid_data, valid_labels, selector, classifiers)\n",
    "rfe_results.to_csv('data/RFE.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = [SequentialFeatureSelector(estimator=RandomForestClassifier(n_estimators=100, max_depth=3), n_features_to_select=100, direction='forward')]\n",
    "sfs_results = full_evaluation(train_data, train_labels, valid_data, valid_labels, selector, classifiers)\n",
    "sfs_results.to_csv('data/SFS.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = [SequentialFeatureSelector(estimator=RandomForestClassifier(n_estimators=100, max_depth=3), n_features_to_select=100, direction='backward')]\n",
    "sbs_results = full_evaluation(train_data, train_labels, valid_data, valid_labels, selector, classifiers)\n",
    "sbs_results.to_csv('data/SBS.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeded methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_df = pd.DataFrame(columns=['Selector', 'Classifier', 'Number_of_Features', 'Accuracy', 'Performance_score'])\n",
    "Cs = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "for C in Cs:\n",
    "    lasso = LogisticRegression(penalty='l1', C=C, solver='liblinear', random_state=0)\n",
    "    lasso.fit(train_data, train_labels)\n",
    "    n_features = sum(lasso.coef_[0] != 0)\n",
    "    score = lasso.score(valid_data, valid_labels)\n",
    "    perf_score = performance_score(score, n_features)\n",
    "    lasso_df = pd.concat([lasso_df, pd.DataFrame({'Selector': ['Lasso'], 'Classifier': ['Lasso'], 'Number_of_Features': [n_features], 'Accuracy': [score], 'Performance_score': [perf_score]})], ignore_index=True)\n",
    "\n",
    "lasso_df.to_csv('data/lasso.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "l1_ratio = [0.9, 0.95, 0.98]\n",
    "elastic_df = pd.DataFrame(columns=['Selector', 'Classifier', 'Number_of_Features', 'Accuracy', 'Performance_score', 'alpha', 'l1_ratio'])\n",
    "for ratio in l1_ratio:\n",
    "    for alpha in alphas:\n",
    "        elastic = ElasticNet(alpha=alpha, l1_ratio=ratio, random_state=0, max_iter = 10000)\n",
    "        elastic.fit(train_data, train_labels)\n",
    "        n_features = sum(elastic.coef_!= 0)\n",
    "        score = elastic.score(valid_data, valid_labels)\n",
    "        perf_score = performance_score(score, n_features)\n",
    "        elastic_df = pd.concat([elastic_df, pd.DataFrame({'Selector': ['Lasso'], 'Classifier': ['Lasso'], 'Number_of_Features': [n_features], 'Accuracy': [score], 'Performance_score': [perf_score], \"alpha\": [alpha], \"l1_ratio\": [ratio]})], ignore_index=True)\n",
    "\n",
    "elastic_df.to_csv('data/elasticNet.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticNet = pd.read_csv('data/elasticNet.csv')\n",
    "elasticNet['Selector'] = 'ElasticNet'\n",
    "elasticNet['Classifier'] = 'ElasticNet'\n",
    "elasticNet.to_csv('data/elasticNet.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = [10, 50, 100, 200, 500, 1000, 2000, 5000, 7000]\n",
    "forest_results = pd.DataFrame()\n",
    "for n in n_features:\n",
    "    selector = [RandomForestSelector(n_features=n)]\n",
    "    df = full_evaluation(train_data, train_labels, valid_data, valid_labels, selector, classifiers)\n",
    "    forest_results = pd.concat([forest_results, df])\n",
    "forest_results.to_csv('data/forest.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = [10, 50, 100, 200, 500, 1000, 2000, 5000, 7000]\n",
    "corr_results = pd.DataFrame()\n",
    "for n in n_features:\n",
    "    selector = [CorrelationSelector(n_features=n)]\n",
    "    df = full_evaluation(train_data, train_labels, valid_data, valid_labels, selector, classifiers)\n",
    "    corr_results = pd.concat([corr_results, df])\n",
    "corr_results.to_csv('data/corr.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = [10, 50, 100, 200, 500, 1000, 2000, 5000, 7000]\n",
    "mutual_results = pd.DataFrame()\n",
    "for n in n_features:\n",
    "    selector = [MutualInformationSelector(n_features=n)]\n",
    "    df = full_evaluation(train_data, train_labels, valid_data, valid_labels, selector, classifiers)\n",
    "    mutual_results = pd.concat([mutual_results, df])\n",
    "mutual_results.to_csv('data/mutual.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select K - Best"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = [10, 50, 100, 200, 500, 1000, 2000, 5000, 7000]\n",
    "anova_results = pd.DataFrame()\n",
    "for n in n_features:\n",
    "    selector = [SelectKBest(f_classif, k=n)]\n",
    "    df = full_evaluation(train_data, train_labels, valid_data, valid_labels, selector, classifiers)\n",
    "    anova_results = pd.concat([anova_results, df])\n",
    "anova_results.to_csv('data/anova.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = [10, 50, 100, 200, 500, 1000, 2000, 5000, 8000]\n",
    "chi2_results = pd.DataFrame()\n",
    "for n in n_features:\n",
    "    selector = [SelectKBest(chi2, k=n)]\n",
    "    df = full_evaluation(train_data, train_labels, valid_data, valid_labels, selector, classifiers)\n",
    "    chi2_results = pd.concat([chi2_results, df])\n",
    "chi2_results.to_csv('data/chi2.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid + wrapper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boruta algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = [BorutaPy(estimator=RandomForestClassifier(n_estimators=100, max_depth=3), n_estimators='auto', verbose=1, random_state=0)]\n",
    "boruta_results = full_evaluation(train_data, train_labels, valid_data, valid_labels, selector, classifiers)\n",
    "boruta_results.to_csv('data/boruta.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector1 = RandomForestSelector(n_features=500)\n",
    "selector2 = RFE(estimator=RandomForestClassifier(n_estimators=100, max_depth=3), n_features_to_select=100, step=1, verbose=1)\n",
    "selectors = [[selector1, selector2]]\n",
    "stack_results = full_evaluation(train_data, train_labels, valid_data, valid_labels, selectors, classifiers)\n",
    "stack_results.to_csv('data/stack.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = [10, 50, 100, 200, 500, 1000, 2000, 5000, 7000]\n",
    "ensemble_results = pd.DataFrame()\n",
    "for n in n_features:\n",
    "    selectors = [RandomForestSelector(n_features=n), SelectKBest(f_classif, k=n), SelectKBest(chi2, k=n), CorrelationSelector(n_features=n), MutualInformationSelector(n_features=n)]\n",
    "    ensemble = [EnsembleSelector(selectors=selectors)]\n",
    "    df = full_evaluation(train_data, train_labels, valid_data, valid_labels, ensemble, classifiers)\n",
    "    ensemble_results = pd.concat([ensemble_results, df])\n",
    "    \n",
    "ensemble_results.to_csv('data/ensemble.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
