{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from boruta import BorutaPy\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, RFE, mutual_info_classif, SelectKBest, f_classif, chi2\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, ElasticNet\n",
    "from feature_selection_package.evaluation import performance_score, single_evaluation, full_evaluation\n",
    "from feature_selection_package.feature_selectors import CorrelationSelector, MutualInformationSelector, RandomForestSelector, EnsembleSelector\n",
    "from boruta import BorutaPy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_word_counts_train_test(train, test):\n",
    "    vectorizer = CountVectorizer()\n",
    "    word_counts_train = vectorizer.fit_transform(train['message'])\n",
    "    word_counts_test = vectorizer.transform(test['message'])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    counts_train_df = pd.DataFrame(word_counts_train.toarray(), columns=feature_names)\n",
    "    counts_test_df = pd.DataFrame(word_counts_test.toarray(), columns=feature_names)\n",
    "    result_train_df = pd.concat([train['label'], counts_train_df], axis=1)\n",
    "    result_test_df = pd.concat([test['label'], counts_test_df], axis=1)\n",
    "    return result_train_df, result_test_df\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data artificial\n",
    "artificial_train_data = pd.read_csv('data/artificial_train.data',header=None,sep=' ').dropna(axis=1)\n",
    "artificial_train_labels = pd.read_csv('data/artificial_train.labels',header=None,sep=' ').dropna(axis=1)\n",
    "artificial_valid_data = pd.read_csv('data/artificial_valid.data',header=None,sep=' ').dropna(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = artificial_train_data.loc[:np.round(artificial_train_data.shape[0]*0.8)]\n",
    "train_labels = artificial_train_labels.loc[:np.round(artificial_train_labels.shape[0]*0.8)].replace(-1,0)\n",
    "valid_data = artificial_train_data.loc[np.round(artificial_train_data.shape[0]*0.8):]\n",
    "valid_labels = artificial_train_labels.loc[np.round(artificial_train_labels.shape[0]*0.8):].replace(-1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Load data sms\n",
    "# sms_train = pd.read_csv('data/sms_train.csv')\n",
    "# sms_train_data, sms_train_labels = sms_train.iloc[:, 1], sms_train.iloc[:, 0]\n",
    "\n",
    "# sms_test_data = pd.read_csv('data/sms_test.csv')\n",
    "# sms_test = sms_test_data.copy()\n",
    "# sms_test['label'] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yo, you at jp and hungry like a mofo?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's é only $140 ard...É rest all ard $180 at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&amp;lt;#&amp;gt; , that's all? Guess that's easy enough</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y?WHERE U AT DOGBREATH? ITS JUST SOUNDING LIKE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Good afternoon sexy buns! How goes the job sea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Tell your friends what you plan to do on Valen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>No. Yes please. Been swimming?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Thank you. I like you as well...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Stupid.its not possible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>The search 4 happiness is 1 of d main sources ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               message\n",
       "0                Yo, you at jp and hungry like a mofo?\n",
       "1    It's é only $140 ard...É rest all ard $180 at ...\n",
       "2     &lt;#&gt; , that's all? Guess that's easy enough\n",
       "3    Y?WHERE U AT DOGBREATH? ITS JUST SOUNDING LIKE...\n",
       "4    Good afternoon sexy buns! How goes the job sea...\n",
       "..                                                 ...\n",
       "995  Tell your friends what you plan to do on Valen...\n",
       "996                     No. Yes please. Been swimming?\n",
       "997                   Thank you. I like you as well...\n",
       "998                            Stupid.its not possible\n",
       "999  The search 4 happiness is 1 of d main sources ...\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sms_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is no use for that dataset\n",
    "# sms = pd.read_csv('data/sms.tsv', header=None, sep='\\t')\n",
    "# sms.columns = ['label', 'message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_sms_train , preprocessed_sms_test = get_word_counts_train_test(sms_train, sms_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = preprocessed_sms_train.iloc[:np.round(len(preprocessed_sms_train)*0.8).astype(int), 1:]\n",
    "# train_labels = preprocessed_sms_train.iloc[:np.round(len(preprocessed_sms_train)*0.8).astype(int), 0]\n",
    "# valid_data = preprocessed_sms_train.iloc[np.round(len(preprocessed_sms_train)*0.8).astype(int):, 1:]\n",
    "# valid_labels = preprocessed_sms_train.iloc[np.round(len(preprocessed_sms_train)*0.8).astype(int):, 0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm1 = SVC(kernel='rbf', C=1, random_state=0)\n",
    "svm2 = SVC(kernel='linear', C=1, random_state=0)\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=0)\n",
    "xgboost = XGBClassifier(learning_rate=0.1, n_estimators=100, max_depth=5, random_state=0)\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=0)\n",
    "\n",
    "logreg = LogisticRegression(penalty='l2', C=1, random_state=0)\n",
    "\n",
    "classifiers = np.array([svm1, svm2, tree, xgboost, rfc, logreg])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:405: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:405: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:405: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:405: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "n_features = [10, 20, 50, 80]\n",
    "pca_results = pd.DataFrame()\n",
    "for n in n_features:\n",
    "    selector = [PCA(n_components=n)]\n",
    "    pca_df = full_evaluation(train_data, train_labels, valid_data, valid_labels, selector, classifiers)\n",
    "    pca_results = pd.concat([pca_results, pca_df])\n",
    "pca_results.to_csv('data2/pca.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:326: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.estimator_.fit(X[:, features], y, **fit_params)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:405: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "n_features = [100, 1000, 7000]\n",
    "selector = [RFE(estimator=RandomForestClassifier(n_estimators=100, max_depth=3), n_features_to_select=7000, step=1, verbose=0)]\n",
    "rfe_results = full_evaluation(train_data, train_labels, valid_data, valid_labels, selector, classifiers)\n",
    "rfe_results.to_csv('data2/RFE.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mikol\\Desktop\\Advanced_Machine_Learning\\Project2\\feature_selection_methods.ipynb Cell 18\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mikol/Desktop/Advanced_Machine_Learning/Project2/feature_selection_methods.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m selector \u001b[39m=\u001b[39m [SequentialFeatureSelector(estimator\u001b[39m=\u001b[39mRandomForestClassifier(n_estimators\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, max_depth\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m), n_features_to_select\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mikol/Desktop/Advanced_Machine_Learning/Project2/feature_selection_methods.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m sfs_results \u001b[39m=\u001b[39m full_evaluation(train_data, train_labels, valid_data, valid_labels, selector, classifiers)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mikol/Desktop/Advanced_Machine_Learning/Project2/feature_selection_methods.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m sfs_results\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mdata/SFS.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\Desktop\\Advanced_Machine_Learning\\Project2\\feature_selection_package\\evaluation.py:78\u001b[0m, in \u001b[0;36mfull_evaluation\u001b[1;34m(X_train, y_train, X_val, y_val, selectors, classifiers)\u001b[0m\n\u001b[0;32m     75\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mSelector\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mClassifier\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mNumber_of_Features\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAccuracy\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPerformance_score\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     77\u001b[0m \u001b[39mfor\u001b[39;00m selector \u001b[39min\u001b[39;00m selectors:\n\u001b[1;32m---> 78\u001b[0m     feature_selection_pipeline \u001b[39m=\u001b[39m feature_selection(X_train, y_train, selector)\n\u001b[0;32m     79\u001b[0m     \u001b[39mfor\u001b[39;00m classifier \u001b[39min\u001b[39;00m classifiers:\n\u001b[0;32m     80\u001b[0m         accuracy, perf_score, n_features \u001b[39m=\u001b[39m single_evaluation(X_train, y_train, X_val, y_val, feature_selection_pipeline, classifier)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\Desktop\\Advanced_Machine_Learning\\Project2\\feature_selection_package\\evaluation.py:33\u001b[0m, in \u001b[0;36mfeature_selection\u001b[1;34m(X, y, selectors, scaler)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     pipeline \u001b[39m=\u001b[39m make_pipeline(scaler, selectors)\n\u001b[1;32m---> 33\u001b[0m pipeline\u001b[39m.\u001b[39;49mfit_transform(X, y) \n\u001b[0;32m     35\u001b[0m \u001b[39mreturn\u001b[39;00m pipeline\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:445\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    443\u001b[0m fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m    444\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(last_step, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 445\u001b[0m     \u001b[39mreturn\u001b[39;00m last_step\u001b[39m.\u001b[39mfit_transform(Xt, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_last_step)\n\u001b[0;32m    446\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    447\u001b[0m     \u001b[39mreturn\u001b[39;00m last_step\u001b[39m.\u001b[39mfit(Xt, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_last_step)\u001b[39m.\u001b[39mtransform(Xt)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 142\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    144\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    145\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    147\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    148\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:862\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m    860\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    861\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m--> 862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_sequential.py:268\u001b[0m, in \u001b[0;36mSequentialFeatureSelector.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    266\u001b[0m is_auto_select \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtol \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_to_select \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    267\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_iterations):\n\u001b[1;32m--> 268\u001b[0m     new_feature_idx, new_score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_best_new_feature_score(\n\u001b[0;32m    269\u001b[0m         cloned_estimator, X, y, current_mask\n\u001b[0;32m    270\u001b[0m     )\n\u001b[0;32m    271\u001b[0m     \u001b[39mif\u001b[39;00m is_auto_select \u001b[39mand\u001b[39;00m ((new_score \u001b[39m-\u001b[39m old_score) \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtol):\n\u001b[0;32m    272\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_sequential.py:299\u001b[0m, in \u001b[0;36mSequentialFeatureSelector._get_best_new_feature_score\u001b[1;34m(self, estimator, X, y, current_mask)\u001b[0m\n\u001b[0;32m    297\u001b[0m         candidate_mask \u001b[39m=\u001b[39m \u001b[39m~\u001b[39mcandidate_mask\n\u001b[0;32m    298\u001b[0m     X_new \u001b[39m=\u001b[39m X[:, candidate_mask]\n\u001b[1;32m--> 299\u001b[0m     scores[feature_idx] \u001b[39m=\u001b[39m cross_val_score(\n\u001b[0;32m    300\u001b[0m         estimator,\n\u001b[0;32m    301\u001b[0m         X_new,\n\u001b[0;32m    302\u001b[0m         y,\n\u001b[0;32m    303\u001b[0m         cv\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcv,\n\u001b[0;32m    304\u001b[0m         scoring\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscoring,\n\u001b[0;32m    305\u001b[0m         n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    306\u001b[0m     )\u001b[39m.\u001b[39mmean()\n\u001b[0;32m    307\u001b[0m new_feature_idx \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(scores, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m feature_idx: scores[feature_idx])\n\u001b[0;32m    308\u001b[0m \u001b[39mreturn\u001b[39;00m new_feature_idx, scores[new_feature_idx]\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    513\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[1;32m--> 515\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[0;32m    516\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m    517\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    518\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    519\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m    520\u001b[0m     scoring\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m\"\u001b[39;49m: scorer},\n\u001b[0;32m    521\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[0;32m    522\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    523\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    524\u001b[0m     fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[0;32m    525\u001b[0m     pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[0;32m    526\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    527\u001b[0m )\n\u001b[0;32m    528\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    268\u001b[0m         clone(estimator),\n\u001b[0;32m    269\u001b[0m         X,\n\u001b[0;32m    270\u001b[0m         y,\n\u001b[0;32m    271\u001b[0m         scorers,\n\u001b[0;32m    272\u001b[0m         train,\n\u001b[0;32m    273\u001b[0m         test,\n\u001b[0;32m    274\u001b[0m         verbose,\n\u001b[0;32m    275\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    276\u001b[0m         fit_params,\n\u001b[0;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[0;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[0;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    281\u001b[0m     )\n\u001b[0;32m    282\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[0;32m    283\u001b[0m )\n\u001b[0;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:473\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    462\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    463\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    464\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    465\u001b[0m ]\n\u001b[0;32m    467\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 473\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    474\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    475\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    476\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    477\u001b[0m )(\n\u001b[0;32m    478\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    479\u001b[0m         t,\n\u001b[0;32m    480\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[0;32m    481\u001b[0m         X,\n\u001b[0;32m    482\u001b[0m         y,\n\u001b[0;32m    483\u001b[0m         sample_weight,\n\u001b[0;32m    484\u001b[0m         i,\n\u001b[0;32m    485\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    486\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    487\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    488\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    489\u001b[0m     )\n\u001b[0;32m    490\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    491\u001b[0m )\n\u001b[0;32m    493\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:184\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    182\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[1;32m--> 184\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    185\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    859\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    860\u001b[0m     \u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \n\u001b[0;32m    862\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 889\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    890\u001b[0m         X,\n\u001b[0;32m    891\u001b[0m         y,\n\u001b[0;32m    892\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    893\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:235\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    233\u001b[0m y_encoded \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(y\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m)\n\u001b[0;32m    234\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_):\n\u001b[1;32m--> 235\u001b[0m     classes_k, y_encoded[:, k] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49munique(y[:, k], return_inverse\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    236\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m.\u001b[39mappend(classes_k)\n\u001b[0;32m    237\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_\u001b[39m.\u001b[39mappend(classes_k\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36munique\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[0;32m    272\u001b[0m ar \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masanyarray(ar)\n\u001b[0;32m    273\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     ret \u001b[39m=\u001b[39m _unique1d(ar, return_index, return_inverse, return_counts, \n\u001b[0;32m    275\u001b[0m                     equal_nan\u001b[39m=\u001b[39;49mequal_nan)\n\u001b[0;32m    276\u001b[0m     \u001b[39mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[0;32m    278\u001b[0m \u001b[39m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\arraysetops.py:355\u001b[0m, in \u001b[0;36m_unique1d\u001b[1;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[0;32m    352\u001b[0m     mask[\u001b[39m1\u001b[39m:] \u001b[39m=\u001b[39m aux[\u001b[39m1\u001b[39m:] \u001b[39m!=\u001b[39m aux[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    354\u001b[0m ret \u001b[39m=\u001b[39m (aux[mask],)\n\u001b[1;32m--> 355\u001b[0m \u001b[39mif\u001b[39;00m return_index:\n\u001b[0;32m    356\u001b[0m     ret \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (perm[mask],)\n\u001b[0;32m    357\u001b[0m \u001b[39mif\u001b[39;00m return_inverse:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "selector = [SequentialFeatureSelector(estimator=RandomForestClassifier(n_estimators=100, max_depth=3), n_features_to_select=100, direction='forward')]\n",
    "sfs_results = full_evaluation(train_data, train_labels, valid_data, valid_labels, selector, classifiers)\n",
    "sfs_results.to_csv('data2/SFS.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = [SequentialFeatureSelector(estimator=RandomForestClassifier(n_estimators=100, max_depth=3), n_features_to_select=100, direction='backward')]\n",
    "sbs_results = full_evaluation(train_data, train_labels, valid_data, valid_labels, selector, classifiers)\n",
    "sbs_results.to_csv('data2/SBS.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeded methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lasso_df = pd.DataFrame(columns=['Selector', 'Classifier', 'Number_of_Features', 'Accuracy', 'Performance_score'])\n",
    "Cs = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "for C in Cs:\n",
    "    lasso = LogisticRegression(penalty='l1', C=C, solver='liblinear', random_state=0)\n",
    "    lasso.fit(train_data, train_labels)\n",
    "    n_features = sum(lasso.coef_[0] != 0)\n",
    "    score = lasso.score(valid_data, valid_labels)\n",
    "    perf_score = performance_score(score, n_features)\n",
    "    lasso_df = pd.concat([lasso_df, pd.DataFrame({'Selector': ['Lasso'], 'Classifier': ['Lasso'], 'Number_of_Features': [n_features], 'Accuracy': [score], 'Performance_score': [perf_score]})], ignore_index=True)\n",
    "\n",
    "lasso_df.to_csv('data2/lasso.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "l1_ratio = [0.9, 0.95, 0.98]\n",
    "elastic_df = pd.DataFrame(columns=['Selector', 'Classifier', 'Number_of_Features', 'Accuracy', 'Performance_score', 'alpha', 'l1_ratio'])\n",
    "for ratio in l1_ratio:\n",
    "    for alpha in alphas:\n",
    "        elastic = ElasticNet(alpha=alpha, l1_ratio=ratio, random_state=0, max_iter = 10000)\n",
    "        elastic.fit(train_data, train_labels)\n",
    "        n_features = sum(elastic.coef_!= 0)\n",
    "        score = elastic.score(valid_data, valid_labels)\n",
    "        perf_score = performance_score(score, n_features)\n",
    "        elastic_df = pd.concat([elastic_df, pd.DataFrame({'Selector': ['Lasso'], 'Classifier': ['Lasso'], 'Number_of_Features': [n_features], 'Accuracy': [score], 'Performance_score': [perf_score], \"alpha\": [alpha], \"l1_ratio\": [ratio]})], ignore_index=True)\n",
    "\n",
    "elastic_df.to_csv('data2/elasticNet.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticNet = pd.read_csv('data2/elasticNet.csv')\n",
    "elasticNet['Selector'] = 'ElasticNet'\n",
    "elasticNet['Classifier'] = 'ElasticNet'\n",
    "elasticNet.to_csv('data2/elasticNet.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikol\\Desktop\\Advanced_Machine_Learning\\Project2\\feature_selection_package\\feature_selectors.py:124: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  clf.fit(X, y)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:405: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\Desktop\\Advanced_Machine_Learning\\Project2\\feature_selection_package\\feature_selectors.py:124: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  clf.fit(X, y)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:405: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\Desktop\\Advanced_Machine_Learning\\Project2\\feature_selection_package\\feature_selectors.py:124: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  clf.fit(X, y)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:405: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\Desktop\\Advanced_Machine_Learning\\Project2\\feature_selection_package\\feature_selectors.py:124: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  clf.fit(X, y)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:405: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\mikol\\Desktop\\Advanced_Machine_Learning\\Project2\\feature_selection_package\\feature_selectors.py:124: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  clf.fit(X, y)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:405: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\mikol\\Desktop\\Advanced_Machine_Learning\\Project2\\feature_selection_package\\feature_selectors.py:124: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  clf.fit(X, y)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:405: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\mikol\\Desktop\\Advanced_Machine_Learning\\Project2\\feature_selection_package\\feature_selectors.py:124: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  clf.fit(X, y)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:405: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\mikol\\Desktop\\Advanced_Machine_Learning\\Project2\\feature_selection_package\\feature_selectors.py:124: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  clf.fit(X, y)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:405: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\mikol\\Desktop\\Advanced_Machine_Learning\\Project2\\feature_selection_package\\feature_selectors.py:124: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  clf.fit(X, y)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:405: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "n_features = [10, 50, 100, 200, 500, 1000, 2000, 5000, 7000]\n",
    "forest_results = pd.DataFrame()\n",
    "for n in n_features:\n",
    "    selector = [RandomForestSelector(n_features=n)]\n",
    "    df = full_evaluation(train_data, train_labels, valid_data, valid_labels, selector, classifiers)\n",
    "    forest_results = pd.concat([forest_results, df])\n",
    "forest_results.to_csv('data2/forest.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mikol\\Desktop\\Advanced_Machine_Learning\\Project2\\feature_selection_methods.ipynb Cell 33\u001b[0m in \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mikol/Desktop/Advanced_Machine_Learning/Project2/feature_selection_methods.ipynb#X42sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m n_features:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mikol/Desktop/Advanced_Machine_Learning/Project2/feature_selection_methods.ipynb#X42sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     selector \u001b[39m=\u001b[39m [CorrelationSelector(n_features\u001b[39m=\u001b[39mn)]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mikol/Desktop/Advanced_Machine_Learning/Project2/feature_selection_methods.ipynb#X42sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     df \u001b[39m=\u001b[39m full_evaluation(train_data, train_labels, valid_data, valid_labels, selector, classifiers)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mikol/Desktop/Advanced_Machine_Learning/Project2/feature_selection_methods.ipynb#X42sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     corr_results \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([corr_results, df])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mikol/Desktop/Advanced_Machine_Learning/Project2/feature_selection_methods.ipynb#X42sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m corr_results\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mdata2/corr.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\Desktop\\Advanced_Machine_Learning\\Project2\\feature_selection_package\\evaluation.py:78\u001b[0m, in \u001b[0;36mfull_evaluation\u001b[1;34m(X_train, y_train, X_val, y_val, selectors, classifiers)\u001b[0m\n\u001b[0;32m     75\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mSelector\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mClassifier\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mNumber_of_Features\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAccuracy\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPerformance_score\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     77\u001b[0m \u001b[39mfor\u001b[39;00m selector \u001b[39min\u001b[39;00m selectors:\n\u001b[1;32m---> 78\u001b[0m     feature_selection_pipeline \u001b[39m=\u001b[39m feature_selection(X_train, y_train, selector)\n\u001b[0;32m     79\u001b[0m     \u001b[39mfor\u001b[39;00m classifier \u001b[39min\u001b[39;00m classifiers:\n\u001b[0;32m     80\u001b[0m         accuracy, perf_score, n_features \u001b[39m=\u001b[39m single_evaluation(X_train, y_train, X_val, y_val, feature_selection_pipeline, classifier)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\Desktop\\Advanced_Machine_Learning\\Project2\\feature_selection_package\\evaluation.py:33\u001b[0m, in \u001b[0;36mfeature_selection\u001b[1;34m(X, y, selectors, scaler)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     pipeline \u001b[39m=\u001b[39m make_pipeline(scaler, selectors)\n\u001b[1;32m---> 33\u001b[0m pipeline\u001b[39m.\u001b[39;49mfit_transform(X, y) \n\u001b[0;32m     35\u001b[0m \u001b[39mreturn\u001b[39;00m pipeline\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:445\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    443\u001b[0m fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m    444\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(last_step, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 445\u001b[0m     \u001b[39mreturn\u001b[39;00m last_step\u001b[39m.\u001b[39mfit_transform(Xt, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_last_step)\n\u001b[0;32m    446\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    447\u001b[0m     \u001b[39mreturn\u001b[39;00m last_step\u001b[39m.\u001b[39mfit(Xt, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_last_step)\u001b[39m.\u001b[39mtransform(Xt)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 142\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    144\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    145\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    147\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    148\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\mikol\\Desktop\\Advanced_Machine_Learning\\Project2\\feature_selection_package\\feature_selectors.py:39\u001b[0m, in \u001b[0;36mCorrelationSelector.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_transform\u001b[39m(\u001b[39mself\u001b[39m, X, y):\n\u001b[1;32m---> 39\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, y)\n\u001b[0;32m     40\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\Desktop\\Advanced_Machine_Learning\\Project2\\feature_selection_package\\feature_selectors.py:29\u001b[0m, in \u001b[0;36mCorrelationSelector.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     27\u001b[0m X \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(X)\n\u001b[0;32m     28\u001b[0m X\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 29\u001b[0m corr \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(X)\u001b[39m.\u001b[39mcorrwith(pd\u001b[39m.\u001b[39;49mSeries(y))\n\u001b[0;32m     30\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures \u001b[39m=\u001b[39m corr\u001b[39m.\u001b[39mabs()\u001b[39m.\u001b[39msort_values(ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39miloc[:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features]\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mto_list()\n\u001b[0;32m     31\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupport_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39mTrue\u001b[39;00m \u001b[39mif\u001b[39;00m feature \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures \u001b[39melse\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m X\u001b[39m.\u001b[39mcolumns])\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:387\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     name \u001b[39m=\u001b[39m ibase\u001b[39m.\u001b[39mmaybe_extract_name(name, data, \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m))\n\u001b[1;32m--> 387\u001b[0m     \u001b[39mif\u001b[39;00m is_empty_data(data) \u001b[39mand\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m         \u001b[39m# gh-17261\u001b[39;00m\n\u001b[0;32m    389\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    390\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe default dtype for empty Series will be \u001b[39m\u001b[39m'\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m'\u001b[39m\u001b[39m instead \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    391\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mof \u001b[39m\u001b[39m'\u001b[39m\u001b[39mfloat64\u001b[39m\u001b[39m'\u001b[39m\u001b[39m in a future version. Specify a dtype explicitly \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m             stacklevel\u001b[39m=\u001b[39mfind_stack_level(inspect\u001b[39m.\u001b[39mcurrentframe()),\n\u001b[0;32m    395\u001b[0m         )\n\u001b[0;32m    396\u001b[0m         \u001b[39m# uncomment the line below when removing the FutureWarning\u001b[39;00m\n\u001b[0;32m    397\u001b[0m         \u001b[39m# dtype = np.dtype(object)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\construction.py:878\u001b[0m, in \u001b[0;36mis_empty_data\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    876\u001b[0m is_none \u001b[39m=\u001b[39m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    877\u001b[0m is_list_like_without_dtype \u001b[39m=\u001b[39m is_list_like(data) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(data, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 878\u001b[0m is_simple_empty \u001b[39m=\u001b[39m is_list_like_without_dtype \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;49;00m data\n\u001b[0;32m    879\u001b[0m \u001b[39mreturn\u001b[39;00m is_none \u001b[39mor\u001b[39;00m is_simple_empty\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:1526\u001b[0m, in \u001b[0;36mNDFrame.__nonzero__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1524\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[0;32m   1525\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__nonzero__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NoReturn:\n\u001b[1;32m-> 1526\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1527\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe truth value of a \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m is ambiguous. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1528\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUse a.empty, a.bool(), a.item(), a.any() or a.all().\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1529\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "n_features = [10, 50, 100, 200, 500, 1000, 2000, 5000, 7000]\n",
    "corr_results = pd.DataFrame()\n",
    "for n in n_features:\n",
    "    selector = [CorrelationSelector(n_features=n)]\n",
    "    df = full_evaluation(train_data, train_labels, valid_data, valid_labels, selector, classifiers)\n",
    "    corr_results = pd.concat([corr_results, df])\n",
    "corr_results.to_csv('data2/corr.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = [10, 50, 100, 200, 500, 1000, 2000, 5000, 7000]\n",
    "mutual_results = pd.DataFrame()\n",
    "for n in n_features:\n",
    "    selector = [MutualInformationSelector(n_features=n)]\n",
    "    df = full_evaluation(train_data, train_labels, valid_data, valid_labels, selector, classifiers)\n",
    "    mutual_results = pd.concat([mutual_results, df])\n",
    "mutual_results.to_csv('data2/mutual.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select K - Best"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [  23   24   30   31   32   55   56   58   68   69   74   83   85   88\n",
      "   91  105  113  119  128  144  165  167  173  178  183  187  189  199\n",
      "  203  205  206  216  221  222  226  238  245  248  251  259  265  270\n",
      "  274  280  292  331  333  335  359  360  371  380  384  411  417  420\n",
      "  423  429  433  460  483  542  544  554  555  567  569  578  592  594\n",
      "  600  601  606  626  627  646  656  666  672  708  714  718  738  746\n",
      "  749  751  769  772  773  778  782  789  791  796  806  814  822  824\n",
      "  826  830  831  836  843  853  857  862  870  871  882  885  899  914\n",
      "  917  923  926  939  949  950  966  971  975  977  985  987  988  998\n",
      "  999 1012 1020 1026 1029 1030 1032 1040 1043 1044 1058 1065 1080 1086\n",
      " 1095 1104 1126 1129 1143 1166 1169 1176 1195 1206 1209 1216 1217 1219\n",
      " 1225 1226 1240 1241 1245 1248 1253 1255 1270 1279 1283 1306 1308 1338\n",
      " 1343 1349 1366 1397 1398 1420 1439 1445 1446 1478 1481 1489 1491 1493\n",
      " 1499 1512 1514 1515 1524 1527 1530 1532 1538 1551 1560 1563 1569 1579\n",
      " 1582 1591 1595 1621 1623 1633 1643 1656 1667 1679 1687 1688 1690 1702\n",
      " 1714 1723 1739 1771 1784 1786 1793 1797 1800 1801 1812 1813 1816 1838\n",
      " 1854 1863 1867 1868 1869 1873 1893 1914 1917 1920 1931 1934 1935 1945\n",
      " 1947 1967 1974 1988 2011 2020 2022 2043 2044 2045 2051 2053 2055 2063\n",
      " 2080 2088 2089 2100 2101 2112 2131 2145 2151 2155 2157 2159 2186 2187\n",
      " 2201 2203 2204 2206 2212 2214 2228 2230 2241 2244 2255 2267 2272 2273\n",
      " 2278 2287 2298 2323 2332 2345 2367 2382 2384 2388 2391 2399 2410 2432\n",
      " 2458 2463 2466 2467 2468 2472 2474 2480 2491 2495 2496 2498 2555 2562\n",
      " 2565 2588 2600 2609 2633 2643 2673 2689 2691 2693 2694 2697 2723 2727\n",
      " 2731 2733 2741 2743 2755 2762 2767 2773 2782 2785 2806 2808 2820 2825\n",
      " 2834 2850 2864 2877 2879 2887 2890 2895 2913 2914 2919 2926 2927 2929\n",
      " 2943 2952 2968 2969 2975 2999 3002 3007 3011 3016 3033 3035 3037 3039\n",
      " 3040 3041 3060 3078 3079 3083 3085 3087 3090 3092 3097 3122 3127 3137\n",
      " 3143 3172 3174 3186 3187 3189 3198 3203 3205 3219 3221 3230 3246 3255\n",
      " 3256 3263 3267 3276 3278 3280 3292 3295 3315 3327 3329 3333 3343 3355\n",
      " 3359 3360 3361 3384 3390 3393 3394 3415 3416 3420 3422 3427 3458 3474\n",
      " 3476 3493 3515 3525 3540 3564 3581 3589 3593 3596 3609 3614 3673 3696\n",
      " 3698 3701 3708 3715 3718 3723 3727 3731 3733 3748 3753 3769 3776 3788\n",
      " 3805 3822 3832 3839 3848 3860 3862 3863 3892 3906 3908 3926 3944 3978\n",
      " 3996 4001 4003 4015 4018 4034 4049 4063 4065 4069 4074 4082 4091 4111\n",
      " 4123 4125 4158 4169 4182 4191 4196 4209 4212 4216 4220 4223 4229 4251\n",
      " 4264 4274 4307 4338 4357 4364 4365 4367 4373 4379 4391 4448 4463 4465\n",
      " 4469 4471 4489 4506 4512 4528 4532 4537 4539 4541 4556 4576 4578 4581\n",
      " 4584 4599 4600 4603 4620 4623 4640 4641 4660 4662 4669 4679 4681 4688\n",
      " 4725 4730 4749 4754 4759 4762 4780 4781 4783 4784 4788 4791 4805 4813\n",
      " 4814 4840 4862 4871 4876 4878 4881 4915 4921 4930 4941 4977 4980 4984\n",
      " 4991 4995 5029 5031 5038 5060 5090 5115 5119 5121 5123 5138 5140 5148\n",
      " 5180 5194 5210 5212 5213 5214 5216 5218 5226 5230 5240 5245 5255 5266\n",
      " 5267 5283 5287 5289 5294 5296 5312 5322 5323 5326 5353 5355 5357 5362\n",
      " 5364 5383 5389 5398 5399 5406 5414 5415 5417 5427 5436 5449 5452 5459\n",
      " 5461 5470 5487 5499 5508 5511 5512 5526 5528 5530 5531 5535 5549 5557\n",
      " 5566 5568 5571 5579 5582 5586 5592 5609 5614 5635 5638 5645 5679 5680\n",
      " 5686 5688 5693 5703 5714 5718 5720 5733 5750 5761 5775 5782 5786 5789\n",
      " 5803 5806 5813 5816 5820 5822 5833 5845 5852 5854 5865 5866 5880 5898\n",
      " 5901 5943 5961 5963 5974 5976 5990 6009 6010 6028 6031 6032 6053 6061\n",
      " 6080 6086 6088 6097 6099 6118 6123 6125 6139 6158 6159 6170 6171 6175\n",
      " 6183 6186 6187 6196 6206 6208 6218 6221 6240 6242 6248 6258 6272 6280\n",
      " 6290 6299 6300 6323 6364 6372 6394 6415 6416 6420 6421 6424 6426 6434\n",
      " 6437 6447 6454 6460 6466 6470 6479 6495 6502 6517 6532 6535 6539 6546\n",
      " 6553 6556 6563 6571 6582 6583 6585 6605 6618 6622 6625 6626 6630 6657\n",
      " 6669 6670 6686 6703 6708 6714 6734 6751 6753 6757 6763 6792 6805 6807\n",
      " 6809 6823 6830 6831 6842 6843 6859 6867 6869 6896 6919 6932 6937 6945\n",
      " 6951 6959 6964 6971 6975 6978 6995 6997 7029 7033 7064 7090 7096 7112\n",
      " 7114 7140 7141 7148 7149 7163 7175 7176 7179 7191 7194 7196 7204 7219\n",
      " 7229 7238 7250 7254 7260 7264 7272 7279 7302 7305 7313 7318 7328 7343\n",
      " 7351 7364 7367 7390 7396 7423 7440 7448 7449 7463 7464 7467 7473 7494\n",
      " 7497 7515 7531 7536 7537 7543 7545 7591 7593 7595 7602 7614 7617 7632\n",
      " 7636 7637 7641 7649 7650 7651 7668 7675 7676 7678 7687 7700 7712 7719\n",
      " 7725 7734 7735 7737 7745 7758 7759 7766 7781 7798 7828 7853 7854 7866\n",
      " 7871 7885 7890 7891 7900] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [  23   24   30   31   32   55   56   58   68   69   74   83   85   88\n",
      "   91  105  113  119  128  144  165  167  173  178  183  187  189  199\n",
      "  203  205  206  216  221  222  226  238  245  248  251  259  265  270\n",
      "  274  280  292  331  333  335  359  360  371  380  384  411  417  420\n",
      "  423  429  433  460  483  542  544  554  555  567  569  578  592  594\n",
      "  600  601  606  626  627  646  656  666  672  708  714  718  738  746\n",
      "  749  751  769  772  773  778  782  789  791  796  806  814  822  824\n",
      "  826  830  831  836  843  853  857  862  870  871  882  885  899  914\n",
      "  917  923  926  939  949  950  966  971  975  977  985  987  988  998\n",
      "  999 1012 1020 1026 1029 1030 1032 1040 1043 1044 1058 1065 1080 1086\n",
      " 1095 1104 1126 1129 1143 1166 1169 1176 1195 1206 1209 1216 1217 1219\n",
      " 1225 1226 1240 1241 1245 1248 1253 1255 1270 1279 1283 1306 1308 1338\n",
      " 1343 1349 1366 1397 1398 1420 1439 1445 1446 1478 1481 1489 1491 1493\n",
      " 1499 1512 1514 1515 1524 1527 1530 1532 1538 1551 1560 1563 1569 1579\n",
      " 1582 1591 1595 1621 1623 1633 1643 1656 1667 1679 1687 1688 1690 1702\n",
      " 1714 1723 1739 1771 1784 1786 1793 1797 1800 1801 1812 1813 1816 1838\n",
      " 1854 1863 1867 1868 1869 1873 1893 1914 1917 1920 1931 1934 1935 1945\n",
      " 1947 1967 1974 1988 2011 2020 2022 2043 2044 2045 2051 2053 2055 2063\n",
      " 2080 2088 2089 2100 2101 2112 2131 2145 2151 2155 2157 2159 2186 2187\n",
      " 2201 2203 2204 2206 2212 2214 2228 2230 2241 2244 2255 2267 2272 2273\n",
      " 2278 2287 2298 2323 2332 2345 2367 2382 2384 2388 2391 2399 2410 2432\n",
      " 2458 2463 2466 2467 2468 2472 2474 2480 2491 2495 2496 2498 2555 2562\n",
      " 2565 2588 2600 2609 2633 2643 2673 2689 2691 2693 2694 2697 2723 2727\n",
      " 2731 2733 2741 2743 2755 2762 2767 2773 2782 2785 2806 2808 2820 2825\n",
      " 2834 2850 2864 2877 2879 2887 2890 2895 2913 2914 2919 2926 2927 2929\n",
      " 2943 2952 2968 2969 2975 2999 3002 3007 3011 3016 3033 3035 3037 3039\n",
      " 3040 3041 3060 3078 3079 3083 3085 3087 3090 3092 3097 3122 3127 3137\n",
      " 3143 3172 3174 3186 3187 3189 3198 3203 3205 3219 3221 3230 3246 3255\n",
      " 3256 3263 3267 3276 3278 3280 3292 3295 3315 3327 3329 3333 3343 3355\n",
      " 3359 3360 3361 3384 3390 3393 3394 3415 3416 3420 3422 3427 3458 3474\n",
      " 3476 3493 3515 3525 3540 3564 3581 3589 3593 3596 3609 3614 3673 3696\n",
      " 3698 3701 3708 3715 3718 3723 3727 3731 3733 3748 3753 3769 3776 3788\n",
      " 3805 3822 3832 3839 3848 3860 3862 3863 3892 3906 3908 3926 3944 3978\n",
      " 3996 4001 4003 4015 4018 4034 4049 4063 4065 4069 4074 4082 4091 4111\n",
      " 4123 4125 4158 4169 4182 4191 4196 4209 4212 4216 4220 4223 4229 4251\n",
      " 4264 4274 4307 4338 4357 4364 4365 4367 4373 4379 4391 4448 4463 4465\n",
      " 4469 4471 4489 4506 4512 4528 4532 4537 4539 4541 4556 4576 4578 4581\n",
      " 4584 4599 4600 4603 4620 4623 4640 4641 4660 4662 4669 4679 4681 4688\n",
      " 4725 4730 4749 4754 4759 4762 4780 4781 4783 4784 4788 4791 4805 4813\n",
      " 4814 4840 4862 4871 4876 4878 4881 4915 4921 4930 4941 4977 4980 4984\n",
      " 4991 4995 5029 5031 5038 5060 5090 5115 5119 5121 5123 5138 5140 5148\n",
      " 5180 5194 5210 5212 5213 5214 5216 5218 5226 5230 5240 5245 5255 5266\n",
      " 5267 5283 5287 5289 5294 5296 5312 5322 5323 5326 5353 5355 5357 5362\n",
      " 5364 5383 5389 5398 5399 5406 5414 5415 5417 5427 5436 5449 5452 5459\n",
      " 5461 5470 5487 5499 5508 5511 5512 5526 5528 5530 5531 5535 5549 5557\n",
      " 5566 5568 5571 5579 5582 5586 5592 5609 5614 5635 5638 5645 5679 5680\n",
      " 5686 5688 5693 5703 5714 5718 5720 5733 5750 5761 5775 5782 5786 5789\n",
      " 5803 5806 5813 5816 5820 5822 5833 5845 5852 5854 5865 5866 5880 5898\n",
      " 5901 5943 5961 5963 5974 5976 5990 6009 6010 6028 6031 6032 6053 6061\n",
      " 6080 6086 6088 6097 6099 6118 6123 6125 6139 6158 6159 6170 6171 6175\n",
      " 6183 6186 6187 6196 6206 6208 6218 6221 6240 6242 6248 6258 6272 6280\n",
      " 6290 6299 6300 6323 6364 6372 6394 6415 6416 6420 6421 6424 6426 6434\n",
      " 6437 6447 6454 6460 6466 6470 6479 6495 6502 6517 6532 6535 6539 6546\n",
      " 6553 6556 6563 6571 6582 6583 6585 6605 6618 6622 6625 6626 6630 6657\n",
      " 6669 6670 6686 6703 6708 6714 6734 6751 6753 6757 6763 6792 6805 6807\n",
      " 6809 6823 6830 6831 6842 6843 6859 6867 6869 6896 6919 6932 6937 6945\n",
      " 6951 6959 6964 6971 6975 6978 6995 6997 7029 7033 7064 7090 7096 7112\n",
      " 7114 7140 7141 7148 7149 7163 7175 7176 7179 7191 7194 7196 7204 7219\n",
      " 7229 7238 7250 7254 7260 7264 7272 7279 7302 7305 7313 7318 7328 7343\n",
      " 7351 7364 7367 7390 7396 7423 7440 7448 7449 7463 7464 7467 7473 7494\n",
      " 7497 7515 7531 7536 7537 7543 7545 7591 7593 7595 7602 7614 7617 7632\n",
      " 7636 7637 7641 7649 7650 7651 7668 7675 7676 7678 7687 7700 7712 7719\n",
      " 7725 7734 7735 7737 7745 7758 7759 7766 7781 7798 7828 7853 7854 7866\n",
      " 7871 7885 7890 7891 7900] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [  23   24   30   31   32   55   56   58   68   69   74   83   85   88\n",
      "   91  105  113  119  128  144  165  167  173  178  183  187  189  199\n",
      "  203  205  206  216  221  222  226  238  245  248  251  259  265  270\n",
      "  274  280  292  331  333  335  359  360  371  380  384  411  417  420\n",
      "  423  429  433  460  483  542  544  554  555  567  569  578  592  594\n",
      "  600  601  606  626  627  646  656  666  672  708  714  718  738  746\n",
      "  749  751  769  772  773  778  782  789  791  796  806  814  822  824\n",
      "  826  830  831  836  843  853  857  862  870  871  882  885  899  914\n",
      "  917  923  926  939  949  950  966  971  975  977  985  987  988  998\n",
      "  999 1012 1020 1026 1029 1030 1032 1040 1043 1044 1058 1065 1080 1086\n",
      " 1095 1104 1126 1129 1143 1166 1169 1176 1195 1206 1209 1216 1217 1219\n",
      " 1225 1226 1240 1241 1245 1248 1253 1255 1270 1279 1283 1306 1308 1338\n",
      " 1343 1349 1366 1397 1398 1420 1439 1445 1446 1478 1481 1489 1491 1493\n",
      " 1499 1512 1514 1515 1524 1527 1530 1532 1538 1551 1560 1563 1569 1579\n",
      " 1582 1591 1595 1621 1623 1633 1643 1656 1667 1679 1687 1688 1690 1702\n",
      " 1714 1723 1739 1771 1784 1786 1793 1797 1800 1801 1812 1813 1816 1838\n",
      " 1854 1863 1867 1868 1869 1873 1893 1914 1917 1920 1931 1934 1935 1945\n",
      " 1947 1967 1974 1988 2011 2020 2022 2043 2044 2045 2051 2053 2055 2063\n",
      " 2080 2088 2089 2100 2101 2112 2131 2145 2151 2155 2157 2159 2186 2187\n",
      " 2201 2203 2204 2206 2212 2214 2228 2230 2241 2244 2255 2267 2272 2273\n",
      " 2278 2287 2298 2323 2332 2345 2367 2382 2384 2388 2391 2399 2410 2432\n",
      " 2458 2463 2466 2467 2468 2472 2474 2480 2491 2495 2496 2498 2555 2562\n",
      " 2565 2588 2600 2609 2633 2643 2673 2689 2691 2693 2694 2697 2723 2727\n",
      " 2731 2733 2741 2743 2755 2762 2767 2773 2782 2785 2806 2808 2820 2825\n",
      " 2834 2850 2864 2877 2879 2887 2890 2895 2913 2914 2919 2926 2927 2929\n",
      " 2943 2952 2968 2969 2975 2999 3002 3007 3011 3016 3033 3035 3037 3039\n",
      " 3040 3041 3060 3078 3079 3083 3085 3087 3090 3092 3097 3122 3127 3137\n",
      " 3143 3172 3174 3186 3187 3189 3198 3203 3205 3219 3221 3230 3246 3255\n",
      " 3256 3263 3267 3276 3278 3280 3292 3295 3315 3327 3329 3333 3343 3355\n",
      " 3359 3360 3361 3384 3390 3393 3394 3415 3416 3420 3422 3427 3458 3474\n",
      " 3476 3493 3515 3525 3540 3564 3581 3589 3593 3596 3609 3614 3673 3696\n",
      " 3698 3701 3708 3715 3718 3723 3727 3731 3733 3748 3753 3769 3776 3788\n",
      " 3805 3822 3832 3839 3848 3860 3862 3863 3892 3906 3908 3926 3944 3978\n",
      " 3996 4001 4003 4015 4018 4034 4049 4063 4065 4069 4074 4082 4091 4111\n",
      " 4123 4125 4158 4169 4182 4191 4196 4209 4212 4216 4220 4223 4229 4251\n",
      " 4264 4274 4307 4338 4357 4364 4365 4367 4373 4379 4391 4448 4463 4465\n",
      " 4469 4471 4489 4506 4512 4528 4532 4537 4539 4541 4556 4576 4578 4581\n",
      " 4584 4599 4600 4603 4620 4623 4640 4641 4660 4662 4669 4679 4681 4688\n",
      " 4725 4730 4749 4754 4759 4762 4780 4781 4783 4784 4788 4791 4805 4813\n",
      " 4814 4840 4862 4871 4876 4878 4881 4915 4921 4930 4941 4977 4980 4984\n",
      " 4991 4995 5029 5031 5038 5060 5090 5115 5119 5121 5123 5138 5140 5148\n",
      " 5180 5194 5210 5212 5213 5214 5216 5218 5226 5230 5240 5245 5255 5266\n",
      " 5267 5283 5287 5289 5294 5296 5312 5322 5323 5326 5353 5355 5357 5362\n",
      " 5364 5383 5389 5398 5399 5406 5414 5415 5417 5427 5436 5449 5452 5459\n",
      " 5461 5470 5487 5499 5508 5511 5512 5526 5528 5530 5531 5535 5549 5557\n",
      " 5566 5568 5571 5579 5582 5586 5592 5609 5614 5635 5638 5645 5679 5680\n",
      " 5686 5688 5693 5703 5714 5718 5720 5733 5750 5761 5775 5782 5786 5789\n",
      " 5803 5806 5813 5816 5820 5822 5833 5845 5852 5854 5865 5866 5880 5898\n",
      " 5901 5943 5961 5963 5974 5976 5990 6009 6010 6028 6031 6032 6053 6061\n",
      " 6080 6086 6088 6097 6099 6118 6123 6125 6139 6158 6159 6170 6171 6175\n",
      " 6183 6186 6187 6196 6206 6208 6218 6221 6240 6242 6248 6258 6272 6280\n",
      " 6290 6299 6300 6323 6364 6372 6394 6415 6416 6420 6421 6424 6426 6434\n",
      " 6437 6447 6454 6460 6466 6470 6479 6495 6502 6517 6532 6535 6539 6546\n",
      " 6553 6556 6563 6571 6582 6583 6585 6605 6618 6622 6625 6626 6630 6657\n",
      " 6669 6670 6686 6703 6708 6714 6734 6751 6753 6757 6763 6792 6805 6807\n",
      " 6809 6823 6830 6831 6842 6843 6859 6867 6869 6896 6919 6932 6937 6945\n",
      " 6951 6959 6964 6971 6975 6978 6995 6997 7029 7033 7064 7090 7096 7112\n",
      " 7114 7140 7141 7148 7149 7163 7175 7176 7179 7191 7194 7196 7204 7219\n",
      " 7229 7238 7250 7254 7260 7264 7272 7279 7302 7305 7313 7318 7328 7343\n",
      " 7351 7364 7367 7390 7396 7423 7440 7448 7449 7463 7464 7467 7473 7494\n",
      " 7497 7515 7531 7536 7537 7543 7545 7591 7593 7595 7602 7614 7617 7632\n",
      " 7636 7637 7641 7649 7650 7651 7668 7675 7676 7678 7687 7700 7712 7719\n",
      " 7725 7734 7735 7737 7745 7758 7759 7766 7781 7798 7828 7853 7854 7866\n",
      " 7871 7885 7890 7891 7900] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [  23   24   30   31   32   55   56   58   68   69   74   83   85   88\n",
      "   91  105  113  119  128  144  165  167  173  178  183  187  189  199\n",
      "  203  205  206  216  221  222  226  238  245  248  251  259  265  270\n",
      "  274  280  292  331  333  335  359  360  371  380  384  411  417  420\n",
      "  423  429  433  460  483  542  544  554  555  567  569  578  592  594\n",
      "  600  601  606  626  627  646  656  666  672  708  714  718  738  746\n",
      "  749  751  769  772  773  778  782  789  791  796  806  814  822  824\n",
      "  826  830  831  836  843  853  857  862  870  871  882  885  899  914\n",
      "  917  923  926  939  949  950  966  971  975  977  985  987  988  998\n",
      "  999 1012 1020 1026 1029 1030 1032 1040 1043 1044 1058 1065 1080 1086\n",
      " 1095 1104 1126 1129 1143 1166 1169 1176 1195 1206 1209 1216 1217 1219\n",
      " 1225 1226 1240 1241 1245 1248 1253 1255 1270 1279 1283 1306 1308 1338\n",
      " 1343 1349 1366 1397 1398 1420 1439 1445 1446 1478 1481 1489 1491 1493\n",
      " 1499 1512 1514 1515 1524 1527 1530 1532 1538 1551 1560 1563 1569 1579\n",
      " 1582 1591 1595 1621 1623 1633 1643 1656 1667 1679 1687 1688 1690 1702\n",
      " 1714 1723 1739 1771 1784 1786 1793 1797 1800 1801 1812 1813 1816 1838\n",
      " 1854 1863 1867 1868 1869 1873 1893 1914 1917 1920 1931 1934 1935 1945\n",
      " 1947 1967 1974 1988 2011 2020 2022 2043 2044 2045 2051 2053 2055 2063\n",
      " 2080 2088 2089 2100 2101 2112 2131 2145 2151 2155 2157 2159 2186 2187\n",
      " 2201 2203 2204 2206 2212 2214 2228 2230 2241 2244 2255 2267 2272 2273\n",
      " 2278 2287 2298 2323 2332 2345 2367 2382 2384 2388 2391 2399 2410 2432\n",
      " 2458 2463 2466 2467 2468 2472 2474 2480 2491 2495 2496 2498 2555 2562\n",
      " 2565 2588 2600 2609 2633 2643 2673 2689 2691 2693 2694 2697 2723 2727\n",
      " 2731 2733 2741 2743 2755 2762 2767 2773 2782 2785 2806 2808 2820 2825\n",
      " 2834 2850 2864 2877 2879 2887 2890 2895 2913 2914 2919 2926 2927 2929\n",
      " 2943 2952 2968 2969 2975 2999 3002 3007 3011 3016 3033 3035 3037 3039\n",
      " 3040 3041 3060 3078 3079 3083 3085 3087 3090 3092 3097 3122 3127 3137\n",
      " 3143 3172 3174 3186 3187 3189 3198 3203 3205 3219 3221 3230 3246 3255\n",
      " 3256 3263 3267 3276 3278 3280 3292 3295 3315 3327 3329 3333 3343 3355\n",
      " 3359 3360 3361 3384 3390 3393 3394 3415 3416 3420 3422 3427 3458 3474\n",
      " 3476 3493 3515 3525 3540 3564 3581 3589 3593 3596 3609 3614 3673 3696\n",
      " 3698 3701 3708 3715 3718 3723 3727 3731 3733 3748 3753 3769 3776 3788\n",
      " 3805 3822 3832 3839 3848 3860 3862 3863 3892 3906 3908 3926 3944 3978\n",
      " 3996 4001 4003 4015 4018 4034 4049 4063 4065 4069 4074 4082 4091 4111\n",
      " 4123 4125 4158 4169 4182 4191 4196 4209 4212 4216 4220 4223 4229 4251\n",
      " 4264 4274 4307 4338 4357 4364 4365 4367 4373 4379 4391 4448 4463 4465\n",
      " 4469 4471 4489 4506 4512 4528 4532 4537 4539 4541 4556 4576 4578 4581\n",
      " 4584 4599 4600 4603 4620 4623 4640 4641 4660 4662 4669 4679 4681 4688\n",
      " 4725 4730 4749 4754 4759 4762 4780 4781 4783 4784 4788 4791 4805 4813\n",
      " 4814 4840 4862 4871 4876 4878 4881 4915 4921 4930 4941 4977 4980 4984\n",
      " 4991 4995 5029 5031 5038 5060 5090 5115 5119 5121 5123 5138 5140 5148\n",
      " 5180 5194 5210 5212 5213 5214 5216 5218 5226 5230 5240 5245 5255 5266\n",
      " 5267 5283 5287 5289 5294 5296 5312 5322 5323 5326 5353 5355 5357 5362\n",
      " 5364 5383 5389 5398 5399 5406 5414 5415 5417 5427 5436 5449 5452 5459\n",
      " 5461 5470 5487 5499 5508 5511 5512 5526 5528 5530 5531 5535 5549 5557\n",
      " 5566 5568 5571 5579 5582 5586 5592 5609 5614 5635 5638 5645 5679 5680\n",
      " 5686 5688 5693 5703 5714 5718 5720 5733 5750 5761 5775 5782 5786 5789\n",
      " 5803 5806 5813 5816 5820 5822 5833 5845 5852 5854 5865 5866 5880 5898\n",
      " 5901 5943 5961 5963 5974 5976 5990 6009 6010 6028 6031 6032 6053 6061\n",
      " 6080 6086 6088 6097 6099 6118 6123 6125 6139 6158 6159 6170 6171 6175\n",
      " 6183 6186 6187 6196 6206 6208 6218 6221 6240 6242 6248 6258 6272 6280\n",
      " 6290 6299 6300 6323 6364 6372 6394 6415 6416 6420 6421 6424 6426 6434\n",
      " 6437 6447 6454 6460 6466 6470 6479 6495 6502 6517 6532 6535 6539 6546\n",
      " 6553 6556 6563 6571 6582 6583 6585 6605 6618 6622 6625 6626 6630 6657\n",
      " 6669 6670 6686 6703 6708 6714 6734 6751 6753 6757 6763 6792 6805 6807\n",
      " 6809 6823 6830 6831 6842 6843 6859 6867 6869 6896 6919 6932 6937 6945\n",
      " 6951 6959 6964 6971 6975 6978 6995 6997 7029 7033 7064 7090 7096 7112\n",
      " 7114 7140 7141 7148 7149 7163 7175 7176 7179 7191 7194 7196 7204 7219\n",
      " 7229 7238 7250 7254 7260 7264 7272 7279 7302 7305 7313 7318 7328 7343\n",
      " 7351 7364 7367 7390 7396 7423 7440 7448 7449 7463 7464 7467 7473 7494\n",
      " 7497 7515 7531 7536 7537 7543 7545 7591 7593 7595 7602 7614 7617 7632\n",
      " 7636 7637 7641 7649 7650 7651 7668 7675 7676 7678 7687 7700 7712 7719\n",
      " 7725 7734 7735 7737 7745 7758 7759 7766 7781 7798 7828 7853 7854 7866\n",
      " 7871 7885 7890 7891 7900] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [  23   24   30   31   32   55   56   58   68   69   74   83   85   88\n",
      "   91  105  113  119  128  144  165  167  173  178  183  187  189  199\n",
      "  203  205  206  216  221  222  226  238  245  248  251  259  265  270\n",
      "  274  280  292  331  333  335  359  360  371  380  384  411  417  420\n",
      "  423  429  433  460  483  542  544  554  555  567  569  578  592  594\n",
      "  600  601  606  626  627  646  656  666  672  708  714  718  738  746\n",
      "  749  751  769  772  773  778  782  789  791  796  806  814  822  824\n",
      "  826  830  831  836  843  853  857  862  870  871  882  885  899  914\n",
      "  917  923  926  939  949  950  966  971  975  977  985  987  988  998\n",
      "  999 1012 1020 1026 1029 1030 1032 1040 1043 1044 1058 1065 1080 1086\n",
      " 1095 1104 1126 1129 1143 1166 1169 1176 1195 1206 1209 1216 1217 1219\n",
      " 1225 1226 1240 1241 1245 1248 1253 1255 1270 1279 1283 1306 1308 1338\n",
      " 1343 1349 1366 1397 1398 1420 1439 1445 1446 1478 1481 1489 1491 1493\n",
      " 1499 1512 1514 1515 1524 1527 1530 1532 1538 1551 1560 1563 1569 1579\n",
      " 1582 1591 1595 1621 1623 1633 1643 1656 1667 1679 1687 1688 1690 1702\n",
      " 1714 1723 1739 1771 1784 1786 1793 1797 1800 1801 1812 1813 1816 1838\n",
      " 1854 1863 1867 1868 1869 1873 1893 1914 1917 1920 1931 1934 1935 1945\n",
      " 1947 1967 1974 1988 2011 2020 2022 2043 2044 2045 2051 2053 2055 2063\n",
      " 2080 2088 2089 2100 2101 2112 2131 2145 2151 2155 2157 2159 2186 2187\n",
      " 2201 2203 2204 2206 2212 2214 2228 2230 2241 2244 2255 2267 2272 2273\n",
      " 2278 2287 2298 2323 2332 2345 2367 2382 2384 2388 2391 2399 2410 2432\n",
      " 2458 2463 2466 2467 2468 2472 2474 2480 2491 2495 2496 2498 2555 2562\n",
      " 2565 2588 2600 2609 2633 2643 2673 2689 2691 2693 2694 2697 2723 2727\n",
      " 2731 2733 2741 2743 2755 2762 2767 2773 2782 2785 2806 2808 2820 2825\n",
      " 2834 2850 2864 2877 2879 2887 2890 2895 2913 2914 2919 2926 2927 2929\n",
      " 2943 2952 2968 2969 2975 2999 3002 3007 3011 3016 3033 3035 3037 3039\n",
      " 3040 3041 3060 3078 3079 3083 3085 3087 3090 3092 3097 3122 3127 3137\n",
      " 3143 3172 3174 3186 3187 3189 3198 3203 3205 3219 3221 3230 3246 3255\n",
      " 3256 3263 3267 3276 3278 3280 3292 3295 3315 3327 3329 3333 3343 3355\n",
      " 3359 3360 3361 3384 3390 3393 3394 3415 3416 3420 3422 3427 3458 3474\n",
      " 3476 3493 3515 3525 3540 3564 3581 3589 3593 3596 3609 3614 3673 3696\n",
      " 3698 3701 3708 3715 3718 3723 3727 3731 3733 3748 3753 3769 3776 3788\n",
      " 3805 3822 3832 3839 3848 3860 3862 3863 3892 3906 3908 3926 3944 3978\n",
      " 3996 4001 4003 4015 4018 4034 4049 4063 4065 4069 4074 4082 4091 4111\n",
      " 4123 4125 4158 4169 4182 4191 4196 4209 4212 4216 4220 4223 4229 4251\n",
      " 4264 4274 4307 4338 4357 4364 4365 4367 4373 4379 4391 4448 4463 4465\n",
      " 4469 4471 4489 4506 4512 4528 4532 4537 4539 4541 4556 4576 4578 4581\n",
      " 4584 4599 4600 4603 4620 4623 4640 4641 4660 4662 4669 4679 4681 4688\n",
      " 4725 4730 4749 4754 4759 4762 4780 4781 4783 4784 4788 4791 4805 4813\n",
      " 4814 4840 4862 4871 4876 4878 4881 4915 4921 4930 4941 4977 4980 4984\n",
      " 4991 4995 5029 5031 5038 5060 5090 5115 5119 5121 5123 5138 5140 5148\n",
      " 5180 5194 5210 5212 5213 5214 5216 5218 5226 5230 5240 5245 5255 5266\n",
      " 5267 5283 5287 5289 5294 5296 5312 5322 5323 5326 5353 5355 5357 5362\n",
      " 5364 5383 5389 5398 5399 5406 5414 5415 5417 5427 5436 5449 5452 5459\n",
      " 5461 5470 5487 5499 5508 5511 5512 5526 5528 5530 5531 5535 5549 5557\n",
      " 5566 5568 5571 5579 5582 5586 5592 5609 5614 5635 5638 5645 5679 5680\n",
      " 5686 5688 5693 5703 5714 5718 5720 5733 5750 5761 5775 5782 5786 5789\n",
      " 5803 5806 5813 5816 5820 5822 5833 5845 5852 5854 5865 5866 5880 5898\n",
      " 5901 5943 5961 5963 5974 5976 5990 6009 6010 6028 6031 6032 6053 6061\n",
      " 6080 6086 6088 6097 6099 6118 6123 6125 6139 6158 6159 6170 6171 6175\n",
      " 6183 6186 6187 6196 6206 6208 6218 6221 6240 6242 6248 6258 6272 6280\n",
      " 6290 6299 6300 6323 6364 6372 6394 6415 6416 6420 6421 6424 6426 6434\n",
      " 6437 6447 6454 6460 6466 6470 6479 6495 6502 6517 6532 6535 6539 6546\n",
      " 6553 6556 6563 6571 6582 6583 6585 6605 6618 6622 6625 6626 6630 6657\n",
      " 6669 6670 6686 6703 6708 6714 6734 6751 6753 6757 6763 6792 6805 6807\n",
      " 6809 6823 6830 6831 6842 6843 6859 6867 6869 6896 6919 6932 6937 6945\n",
      " 6951 6959 6964 6971 6975 6978 6995 6997 7029 7033 7064 7090 7096 7112\n",
      " 7114 7140 7141 7148 7149 7163 7175 7176 7179 7191 7194 7196 7204 7219\n",
      " 7229 7238 7250 7254 7260 7264 7272 7279 7302 7305 7313 7318 7328 7343\n",
      " 7351 7364 7367 7390 7396 7423 7440 7448 7449 7463 7464 7467 7473 7494\n",
      " 7497 7515 7531 7536 7537 7543 7545 7591 7593 7595 7602 7614 7617 7632\n",
      " 7636 7637 7641 7649 7650 7651 7668 7675 7676 7678 7687 7700 7712 7719\n",
      " 7725 7734 7735 7737 7745 7758 7759 7766 7781 7798 7828 7853 7854 7866\n",
      " 7871 7885 7890 7891 7900] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [  23   24   30   31   32   55   56   58   68   69   74   83   85   88\n",
      "   91  105  113  119  128  144  165  167  173  178  183  187  189  199\n",
      "  203  205  206  216  221  222  226  238  245  248  251  259  265  270\n",
      "  274  280  292  331  333  335  359  360  371  380  384  411  417  420\n",
      "  423  429  433  460  483  542  544  554  555  567  569  578  592  594\n",
      "  600  601  606  626  627  646  656  666  672  708  714  718  738  746\n",
      "  749  751  769  772  773  778  782  789  791  796  806  814  822  824\n",
      "  826  830  831  836  843  853  857  862  870  871  882  885  899  914\n",
      "  917  923  926  939  949  950  966  971  975  977  985  987  988  998\n",
      "  999 1012 1020 1026 1029 1030 1032 1040 1043 1044 1058 1065 1080 1086\n",
      " 1095 1104 1126 1129 1143 1166 1169 1176 1195 1206 1209 1216 1217 1219\n",
      " 1225 1226 1240 1241 1245 1248 1253 1255 1270 1279 1283 1306 1308 1338\n",
      " 1343 1349 1366 1397 1398 1420 1439 1445 1446 1478 1481 1489 1491 1493\n",
      " 1499 1512 1514 1515 1524 1527 1530 1532 1538 1551 1560 1563 1569 1579\n",
      " 1582 1591 1595 1621 1623 1633 1643 1656 1667 1679 1687 1688 1690 1702\n",
      " 1714 1723 1739 1771 1784 1786 1793 1797 1800 1801 1812 1813 1816 1838\n",
      " 1854 1863 1867 1868 1869 1873 1893 1914 1917 1920 1931 1934 1935 1945\n",
      " 1947 1967 1974 1988 2011 2020 2022 2043 2044 2045 2051 2053 2055 2063\n",
      " 2080 2088 2089 2100 2101 2112 2131 2145 2151 2155 2157 2159 2186 2187\n",
      " 2201 2203 2204 2206 2212 2214 2228 2230 2241 2244 2255 2267 2272 2273\n",
      " 2278 2287 2298 2323 2332 2345 2367 2382 2384 2388 2391 2399 2410 2432\n",
      " 2458 2463 2466 2467 2468 2472 2474 2480 2491 2495 2496 2498 2555 2562\n",
      " 2565 2588 2600 2609 2633 2643 2673 2689 2691 2693 2694 2697 2723 2727\n",
      " 2731 2733 2741 2743 2755 2762 2767 2773 2782 2785 2806 2808 2820 2825\n",
      " 2834 2850 2864 2877 2879 2887 2890 2895 2913 2914 2919 2926 2927 2929\n",
      " 2943 2952 2968 2969 2975 2999 3002 3007 3011 3016 3033 3035 3037 3039\n",
      " 3040 3041 3060 3078 3079 3083 3085 3087 3090 3092 3097 3122 3127 3137\n",
      " 3143 3172 3174 3186 3187 3189 3198 3203 3205 3219 3221 3230 3246 3255\n",
      " 3256 3263 3267 3276 3278 3280 3292 3295 3315 3327 3329 3333 3343 3355\n",
      " 3359 3360 3361 3384 3390 3393 3394 3415 3416 3420 3422 3427 3458 3474\n",
      " 3476 3493 3515 3525 3540 3564 3581 3589 3593 3596 3609 3614 3673 3696\n",
      " 3698 3701 3708 3715 3718 3723 3727 3731 3733 3748 3753 3769 3776 3788\n",
      " 3805 3822 3832 3839 3848 3860 3862 3863 3892 3906 3908 3926 3944 3978\n",
      " 3996 4001 4003 4015 4018 4034 4049 4063 4065 4069 4074 4082 4091 4111\n",
      " 4123 4125 4158 4169 4182 4191 4196 4209 4212 4216 4220 4223 4229 4251\n",
      " 4264 4274 4307 4338 4357 4364 4365 4367 4373 4379 4391 4448 4463 4465\n",
      " 4469 4471 4489 4506 4512 4528 4532 4537 4539 4541 4556 4576 4578 4581\n",
      " 4584 4599 4600 4603 4620 4623 4640 4641 4660 4662 4669 4679 4681 4688\n",
      " 4725 4730 4749 4754 4759 4762 4780 4781 4783 4784 4788 4791 4805 4813\n",
      " 4814 4840 4862 4871 4876 4878 4881 4915 4921 4930 4941 4977 4980 4984\n",
      " 4991 4995 5029 5031 5038 5060 5090 5115 5119 5121 5123 5138 5140 5148\n",
      " 5180 5194 5210 5212 5213 5214 5216 5218 5226 5230 5240 5245 5255 5266\n",
      " 5267 5283 5287 5289 5294 5296 5312 5322 5323 5326 5353 5355 5357 5362\n",
      " 5364 5383 5389 5398 5399 5406 5414 5415 5417 5427 5436 5449 5452 5459\n",
      " 5461 5470 5487 5499 5508 5511 5512 5526 5528 5530 5531 5535 5549 5557\n",
      " 5566 5568 5571 5579 5582 5586 5592 5609 5614 5635 5638 5645 5679 5680\n",
      " 5686 5688 5693 5703 5714 5718 5720 5733 5750 5761 5775 5782 5786 5789\n",
      " 5803 5806 5813 5816 5820 5822 5833 5845 5852 5854 5865 5866 5880 5898\n",
      " 5901 5943 5961 5963 5974 5976 5990 6009 6010 6028 6031 6032 6053 6061\n",
      " 6080 6086 6088 6097 6099 6118 6123 6125 6139 6158 6159 6170 6171 6175\n",
      " 6183 6186 6187 6196 6206 6208 6218 6221 6240 6242 6248 6258 6272 6280\n",
      " 6290 6299 6300 6323 6364 6372 6394 6415 6416 6420 6421 6424 6426 6434\n",
      " 6437 6447 6454 6460 6466 6470 6479 6495 6502 6517 6532 6535 6539 6546\n",
      " 6553 6556 6563 6571 6582 6583 6585 6605 6618 6622 6625 6626 6630 6657\n",
      " 6669 6670 6686 6703 6708 6714 6734 6751 6753 6757 6763 6792 6805 6807\n",
      " 6809 6823 6830 6831 6842 6843 6859 6867 6869 6896 6919 6932 6937 6945\n",
      " 6951 6959 6964 6971 6975 6978 6995 6997 7029 7033 7064 7090 7096 7112\n",
      " 7114 7140 7141 7148 7149 7163 7175 7176 7179 7191 7194 7196 7204 7219\n",
      " 7229 7238 7250 7254 7260 7264 7272 7279 7302 7305 7313 7318 7328 7343\n",
      " 7351 7364 7367 7390 7396 7423 7440 7448 7449 7463 7464 7467 7473 7494\n",
      " 7497 7515 7531 7536 7537 7543 7545 7591 7593 7595 7602 7614 7617 7632\n",
      " 7636 7637 7641 7649 7650 7651 7668 7675 7676 7678 7687 7700 7712 7719\n",
      " 7725 7734 7735 7737 7745 7758 7759 7766 7781 7798 7828 7853 7854 7866\n",
      " 7871 7885 7890 7891 7900] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [  23   24   30   31   32   55   56   58   68   69   74   83   85   88\n",
      "   91  105  113  119  128  144  165  167  173  178  183  187  189  199\n",
      "  203  205  206  216  221  222  226  238  245  248  251  259  265  270\n",
      "  274  280  292  331  333  335  359  360  371  380  384  411  417  420\n",
      "  423  429  433  460  483  542  544  554  555  567  569  578  592  594\n",
      "  600  601  606  626  627  646  656  666  672  708  714  718  738  746\n",
      "  749  751  769  772  773  778  782  789  791  796  806  814  822  824\n",
      "  826  830  831  836  843  853  857  862  870  871  882  885  899  914\n",
      "  917  923  926  939  949  950  966  971  975  977  985  987  988  998\n",
      "  999 1012 1020 1026 1029 1030 1032 1040 1043 1044 1058 1065 1080 1086\n",
      " 1095 1104 1126 1129 1143 1166 1169 1176 1195 1206 1209 1216 1217 1219\n",
      " 1225 1226 1240 1241 1245 1248 1253 1255 1270 1279 1283 1306 1308 1338\n",
      " 1343 1349 1366 1397 1398 1420 1439 1445 1446 1478 1481 1489 1491 1493\n",
      " 1499 1512 1514 1515 1524 1527 1530 1532 1538 1551 1560 1563 1569 1579\n",
      " 1582 1591 1595 1621 1623 1633 1643 1656 1667 1679 1687 1688 1690 1702\n",
      " 1714 1723 1739 1771 1784 1786 1793 1797 1800 1801 1812 1813 1816 1838\n",
      " 1854 1863 1867 1868 1869 1873 1893 1914 1917 1920 1931 1934 1935 1945\n",
      " 1947 1967 1974 1988 2011 2020 2022 2043 2044 2045 2051 2053 2055 2063\n",
      " 2080 2088 2089 2100 2101 2112 2131 2145 2151 2155 2157 2159 2186 2187\n",
      " 2201 2203 2204 2206 2212 2214 2228 2230 2241 2244 2255 2267 2272 2273\n",
      " 2278 2287 2298 2323 2332 2345 2367 2382 2384 2388 2391 2399 2410 2432\n",
      " 2458 2463 2466 2467 2468 2472 2474 2480 2491 2495 2496 2498 2555 2562\n",
      " 2565 2588 2600 2609 2633 2643 2673 2689 2691 2693 2694 2697 2723 2727\n",
      " 2731 2733 2741 2743 2755 2762 2767 2773 2782 2785 2806 2808 2820 2825\n",
      " 2834 2850 2864 2877 2879 2887 2890 2895 2913 2914 2919 2926 2927 2929\n",
      " 2943 2952 2968 2969 2975 2999 3002 3007 3011 3016 3033 3035 3037 3039\n",
      " 3040 3041 3060 3078 3079 3083 3085 3087 3090 3092 3097 3122 3127 3137\n",
      " 3143 3172 3174 3186 3187 3189 3198 3203 3205 3219 3221 3230 3246 3255\n",
      " 3256 3263 3267 3276 3278 3280 3292 3295 3315 3327 3329 3333 3343 3355\n",
      " 3359 3360 3361 3384 3390 3393 3394 3415 3416 3420 3422 3427 3458 3474\n",
      " 3476 3493 3515 3525 3540 3564 3581 3589 3593 3596 3609 3614 3673 3696\n",
      " 3698 3701 3708 3715 3718 3723 3727 3731 3733 3748 3753 3769 3776 3788\n",
      " 3805 3822 3832 3839 3848 3860 3862 3863 3892 3906 3908 3926 3944 3978\n",
      " 3996 4001 4003 4015 4018 4034 4049 4063 4065 4069 4074 4082 4091 4111\n",
      " 4123 4125 4158 4169 4182 4191 4196 4209 4212 4216 4220 4223 4229 4251\n",
      " 4264 4274 4307 4338 4357 4364 4365 4367 4373 4379 4391 4448 4463 4465\n",
      " 4469 4471 4489 4506 4512 4528 4532 4537 4539 4541 4556 4576 4578 4581\n",
      " 4584 4599 4600 4603 4620 4623 4640 4641 4660 4662 4669 4679 4681 4688\n",
      " 4725 4730 4749 4754 4759 4762 4780 4781 4783 4784 4788 4791 4805 4813\n",
      " 4814 4840 4862 4871 4876 4878 4881 4915 4921 4930 4941 4977 4980 4984\n",
      " 4991 4995 5029 5031 5038 5060 5090 5115 5119 5121 5123 5138 5140 5148\n",
      " 5180 5194 5210 5212 5213 5214 5216 5218 5226 5230 5240 5245 5255 5266\n",
      " 5267 5283 5287 5289 5294 5296 5312 5322 5323 5326 5353 5355 5357 5362\n",
      " 5364 5383 5389 5398 5399 5406 5414 5415 5417 5427 5436 5449 5452 5459\n",
      " 5461 5470 5487 5499 5508 5511 5512 5526 5528 5530 5531 5535 5549 5557\n",
      " 5566 5568 5571 5579 5582 5586 5592 5609 5614 5635 5638 5645 5679 5680\n",
      " 5686 5688 5693 5703 5714 5718 5720 5733 5750 5761 5775 5782 5786 5789\n",
      " 5803 5806 5813 5816 5820 5822 5833 5845 5852 5854 5865 5866 5880 5898\n",
      " 5901 5943 5961 5963 5974 5976 5990 6009 6010 6028 6031 6032 6053 6061\n",
      " 6080 6086 6088 6097 6099 6118 6123 6125 6139 6158 6159 6170 6171 6175\n",
      " 6183 6186 6187 6196 6206 6208 6218 6221 6240 6242 6248 6258 6272 6280\n",
      " 6290 6299 6300 6323 6364 6372 6394 6415 6416 6420 6421 6424 6426 6434\n",
      " 6437 6447 6454 6460 6466 6470 6479 6495 6502 6517 6532 6535 6539 6546\n",
      " 6553 6556 6563 6571 6582 6583 6585 6605 6618 6622 6625 6626 6630 6657\n",
      " 6669 6670 6686 6703 6708 6714 6734 6751 6753 6757 6763 6792 6805 6807\n",
      " 6809 6823 6830 6831 6842 6843 6859 6867 6869 6896 6919 6932 6937 6945\n",
      " 6951 6959 6964 6971 6975 6978 6995 6997 7029 7033 7064 7090 7096 7112\n",
      " 7114 7140 7141 7148 7149 7163 7175 7176 7179 7191 7194 7196 7204 7219\n",
      " 7229 7238 7250 7254 7260 7264 7272 7279 7302 7305 7313 7318 7328 7343\n",
      " 7351 7364 7367 7390 7396 7423 7440 7448 7449 7463 7464 7467 7473 7494\n",
      " 7497 7515 7531 7536 7537 7543 7545 7591 7593 7595 7602 7614 7617 7632\n",
      " 7636 7637 7641 7649 7650 7651 7668 7675 7676 7678 7687 7700 7712 7719\n",
      " 7725 7734 7735 7737 7745 7758 7759 7766 7781 7798 7828 7853 7854 7866\n",
      " 7871 7885 7890 7891 7900] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [  23   24   30   31   32   55   56   58   68   69   74   83   85   88\n",
      "   91  105  113  119  128  144  165  167  173  178  183  187  189  199\n",
      "  203  205  206  216  221  222  226  238  245  248  251  259  265  270\n",
      "  274  280  292  331  333  335  359  360  371  380  384  411  417  420\n",
      "  423  429  433  460  483  542  544  554  555  567  569  578  592  594\n",
      "  600  601  606  626  627  646  656  666  672  708  714  718  738  746\n",
      "  749  751  769  772  773  778  782  789  791  796  806  814  822  824\n",
      "  826  830  831  836  843  853  857  862  870  871  882  885  899  914\n",
      "  917  923  926  939  949  950  966  971  975  977  985  987  988  998\n",
      "  999 1012 1020 1026 1029 1030 1032 1040 1043 1044 1058 1065 1080 1086\n",
      " 1095 1104 1126 1129 1143 1166 1169 1176 1195 1206 1209 1216 1217 1219\n",
      " 1225 1226 1240 1241 1245 1248 1253 1255 1270 1279 1283 1306 1308 1338\n",
      " 1343 1349 1366 1397 1398 1420 1439 1445 1446 1478 1481 1489 1491 1493\n",
      " 1499 1512 1514 1515 1524 1527 1530 1532 1538 1551 1560 1563 1569 1579\n",
      " 1582 1591 1595 1621 1623 1633 1643 1656 1667 1679 1687 1688 1690 1702\n",
      " 1714 1723 1739 1771 1784 1786 1793 1797 1800 1801 1812 1813 1816 1838\n",
      " 1854 1863 1867 1868 1869 1873 1893 1914 1917 1920 1931 1934 1935 1945\n",
      " 1947 1967 1974 1988 2011 2020 2022 2043 2044 2045 2051 2053 2055 2063\n",
      " 2080 2088 2089 2100 2101 2112 2131 2145 2151 2155 2157 2159 2186 2187\n",
      " 2201 2203 2204 2206 2212 2214 2228 2230 2241 2244 2255 2267 2272 2273\n",
      " 2278 2287 2298 2323 2332 2345 2367 2382 2384 2388 2391 2399 2410 2432\n",
      " 2458 2463 2466 2467 2468 2472 2474 2480 2491 2495 2496 2498 2555 2562\n",
      " 2565 2588 2600 2609 2633 2643 2673 2689 2691 2693 2694 2697 2723 2727\n",
      " 2731 2733 2741 2743 2755 2762 2767 2773 2782 2785 2806 2808 2820 2825\n",
      " 2834 2850 2864 2877 2879 2887 2890 2895 2913 2914 2919 2926 2927 2929\n",
      " 2943 2952 2968 2969 2975 2999 3002 3007 3011 3016 3033 3035 3037 3039\n",
      " 3040 3041 3060 3078 3079 3083 3085 3087 3090 3092 3097 3122 3127 3137\n",
      " 3143 3172 3174 3186 3187 3189 3198 3203 3205 3219 3221 3230 3246 3255\n",
      " 3256 3263 3267 3276 3278 3280 3292 3295 3315 3327 3329 3333 3343 3355\n",
      " 3359 3360 3361 3384 3390 3393 3394 3415 3416 3420 3422 3427 3458 3474\n",
      " 3476 3493 3515 3525 3540 3564 3581 3589 3593 3596 3609 3614 3673 3696\n",
      " 3698 3701 3708 3715 3718 3723 3727 3731 3733 3748 3753 3769 3776 3788\n",
      " 3805 3822 3832 3839 3848 3860 3862 3863 3892 3906 3908 3926 3944 3978\n",
      " 3996 4001 4003 4015 4018 4034 4049 4063 4065 4069 4074 4082 4091 4111\n",
      " 4123 4125 4158 4169 4182 4191 4196 4209 4212 4216 4220 4223 4229 4251\n",
      " 4264 4274 4307 4338 4357 4364 4365 4367 4373 4379 4391 4448 4463 4465\n",
      " 4469 4471 4489 4506 4512 4528 4532 4537 4539 4541 4556 4576 4578 4581\n",
      " 4584 4599 4600 4603 4620 4623 4640 4641 4660 4662 4669 4679 4681 4688\n",
      " 4725 4730 4749 4754 4759 4762 4780 4781 4783 4784 4788 4791 4805 4813\n",
      " 4814 4840 4862 4871 4876 4878 4881 4915 4921 4930 4941 4977 4980 4984\n",
      " 4991 4995 5029 5031 5038 5060 5090 5115 5119 5121 5123 5138 5140 5148\n",
      " 5180 5194 5210 5212 5213 5214 5216 5218 5226 5230 5240 5245 5255 5266\n",
      " 5267 5283 5287 5289 5294 5296 5312 5322 5323 5326 5353 5355 5357 5362\n",
      " 5364 5383 5389 5398 5399 5406 5414 5415 5417 5427 5436 5449 5452 5459\n",
      " 5461 5470 5487 5499 5508 5511 5512 5526 5528 5530 5531 5535 5549 5557\n",
      " 5566 5568 5571 5579 5582 5586 5592 5609 5614 5635 5638 5645 5679 5680\n",
      " 5686 5688 5693 5703 5714 5718 5720 5733 5750 5761 5775 5782 5786 5789\n",
      " 5803 5806 5813 5816 5820 5822 5833 5845 5852 5854 5865 5866 5880 5898\n",
      " 5901 5943 5961 5963 5974 5976 5990 6009 6010 6028 6031 6032 6053 6061\n",
      " 6080 6086 6088 6097 6099 6118 6123 6125 6139 6158 6159 6170 6171 6175\n",
      " 6183 6186 6187 6196 6206 6208 6218 6221 6240 6242 6248 6258 6272 6280\n",
      " 6290 6299 6300 6323 6364 6372 6394 6415 6416 6420 6421 6424 6426 6434\n",
      " 6437 6447 6454 6460 6466 6470 6479 6495 6502 6517 6532 6535 6539 6546\n",
      " 6553 6556 6563 6571 6582 6583 6585 6605 6618 6622 6625 6626 6630 6657\n",
      " 6669 6670 6686 6703 6708 6714 6734 6751 6753 6757 6763 6792 6805 6807\n",
      " 6809 6823 6830 6831 6842 6843 6859 6867 6869 6896 6919 6932 6937 6945\n",
      " 6951 6959 6964 6971 6975 6978 6995 6997 7029 7033 7064 7090 7096 7112\n",
      " 7114 7140 7141 7148 7149 7163 7175 7176 7179 7191 7194 7196 7204 7219\n",
      " 7229 7238 7250 7254 7260 7264 7272 7279 7302 7305 7313 7318 7328 7343\n",
      " 7351 7364 7367 7390 7396 7423 7440 7448 7449 7463 7464 7467 7473 7494\n",
      " 7497 7515 7531 7536 7537 7543 7545 7591 7593 7595 7602 7614 7617 7632\n",
      " 7636 7637 7641 7649 7650 7651 7668 7675 7676 7678 7687 7700 7712 7719\n",
      " 7725 7734 7735 7737 7745 7758 7759 7766 7781 7798 7828 7853 7854 7866\n",
      " 7871 7885 7890 7891 7900] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [  23   24   30   31   32   55   56   58   68   69   74   83   85   88\n",
      "   91  105  113  119  128  144  165  167  173  178  183  187  189  199\n",
      "  203  205  206  216  221  222  226  238  245  248  251  259  265  270\n",
      "  274  280  292  331  333  335  359  360  371  380  384  411  417  420\n",
      "  423  429  433  460  483  542  544  554  555  567  569  578  592  594\n",
      "  600  601  606  626  627  646  656  666  672  708  714  718  738  746\n",
      "  749  751  769  772  773  778  782  789  791  796  806  814  822  824\n",
      "  826  830  831  836  843  853  857  862  870  871  882  885  899  914\n",
      "  917  923  926  939  949  950  966  971  975  977  985  987  988  998\n",
      "  999 1012 1020 1026 1029 1030 1032 1040 1043 1044 1058 1065 1080 1086\n",
      " 1095 1104 1126 1129 1143 1166 1169 1176 1195 1206 1209 1216 1217 1219\n",
      " 1225 1226 1240 1241 1245 1248 1253 1255 1270 1279 1283 1306 1308 1338\n",
      " 1343 1349 1366 1397 1398 1420 1439 1445 1446 1478 1481 1489 1491 1493\n",
      " 1499 1512 1514 1515 1524 1527 1530 1532 1538 1551 1560 1563 1569 1579\n",
      " 1582 1591 1595 1621 1623 1633 1643 1656 1667 1679 1687 1688 1690 1702\n",
      " 1714 1723 1739 1771 1784 1786 1793 1797 1800 1801 1812 1813 1816 1838\n",
      " 1854 1863 1867 1868 1869 1873 1893 1914 1917 1920 1931 1934 1935 1945\n",
      " 1947 1967 1974 1988 2011 2020 2022 2043 2044 2045 2051 2053 2055 2063\n",
      " 2080 2088 2089 2100 2101 2112 2131 2145 2151 2155 2157 2159 2186 2187\n",
      " 2201 2203 2204 2206 2212 2214 2228 2230 2241 2244 2255 2267 2272 2273\n",
      " 2278 2287 2298 2323 2332 2345 2367 2382 2384 2388 2391 2399 2410 2432\n",
      " 2458 2463 2466 2467 2468 2472 2474 2480 2491 2495 2496 2498 2555 2562\n",
      " 2565 2588 2600 2609 2633 2643 2673 2689 2691 2693 2694 2697 2723 2727\n",
      " 2731 2733 2741 2743 2755 2762 2767 2773 2782 2785 2806 2808 2820 2825\n",
      " 2834 2850 2864 2877 2879 2887 2890 2895 2913 2914 2919 2926 2927 2929\n",
      " 2943 2952 2968 2969 2975 2999 3002 3007 3011 3016 3033 3035 3037 3039\n",
      " 3040 3041 3060 3078 3079 3083 3085 3087 3090 3092 3097 3122 3127 3137\n",
      " 3143 3172 3174 3186 3187 3189 3198 3203 3205 3219 3221 3230 3246 3255\n",
      " 3256 3263 3267 3276 3278 3280 3292 3295 3315 3327 3329 3333 3343 3355\n",
      " 3359 3360 3361 3384 3390 3393 3394 3415 3416 3420 3422 3427 3458 3474\n",
      " 3476 3493 3515 3525 3540 3564 3581 3589 3593 3596 3609 3614 3673 3696\n",
      " 3698 3701 3708 3715 3718 3723 3727 3731 3733 3748 3753 3769 3776 3788\n",
      " 3805 3822 3832 3839 3848 3860 3862 3863 3892 3906 3908 3926 3944 3978\n",
      " 3996 4001 4003 4015 4018 4034 4049 4063 4065 4069 4074 4082 4091 4111\n",
      " 4123 4125 4158 4169 4182 4191 4196 4209 4212 4216 4220 4223 4229 4251\n",
      " 4264 4274 4307 4338 4357 4364 4365 4367 4373 4379 4391 4448 4463 4465\n",
      " 4469 4471 4489 4506 4512 4528 4532 4537 4539 4541 4556 4576 4578 4581\n",
      " 4584 4599 4600 4603 4620 4623 4640 4641 4660 4662 4669 4679 4681 4688\n",
      " 4725 4730 4749 4754 4759 4762 4780 4781 4783 4784 4788 4791 4805 4813\n",
      " 4814 4840 4862 4871 4876 4878 4881 4915 4921 4930 4941 4977 4980 4984\n",
      " 4991 4995 5029 5031 5038 5060 5090 5115 5119 5121 5123 5138 5140 5148\n",
      " 5180 5194 5210 5212 5213 5214 5216 5218 5226 5230 5240 5245 5255 5266\n",
      " 5267 5283 5287 5289 5294 5296 5312 5322 5323 5326 5353 5355 5357 5362\n",
      " 5364 5383 5389 5398 5399 5406 5414 5415 5417 5427 5436 5449 5452 5459\n",
      " 5461 5470 5487 5499 5508 5511 5512 5526 5528 5530 5531 5535 5549 5557\n",
      " 5566 5568 5571 5579 5582 5586 5592 5609 5614 5635 5638 5645 5679 5680\n",
      " 5686 5688 5693 5703 5714 5718 5720 5733 5750 5761 5775 5782 5786 5789\n",
      " 5803 5806 5813 5816 5820 5822 5833 5845 5852 5854 5865 5866 5880 5898\n",
      " 5901 5943 5961 5963 5974 5976 5990 6009 6010 6028 6031 6032 6053 6061\n",
      " 6080 6086 6088 6097 6099 6118 6123 6125 6139 6158 6159 6170 6171 6175\n",
      " 6183 6186 6187 6196 6206 6208 6218 6221 6240 6242 6248 6258 6272 6280\n",
      " 6290 6299 6300 6323 6364 6372 6394 6415 6416 6420 6421 6424 6426 6434\n",
      " 6437 6447 6454 6460 6466 6470 6479 6495 6502 6517 6532 6535 6539 6546\n",
      " 6553 6556 6563 6571 6582 6583 6585 6605 6618 6622 6625 6626 6630 6657\n",
      " 6669 6670 6686 6703 6708 6714 6734 6751 6753 6757 6763 6792 6805 6807\n",
      " 6809 6823 6830 6831 6842 6843 6859 6867 6869 6896 6919 6932 6937 6945\n",
      " 6951 6959 6964 6971 6975 6978 6995 6997 7029 7033 7064 7090 7096 7112\n",
      " 7114 7140 7141 7148 7149 7163 7175 7176 7179 7191 7194 7196 7204 7219\n",
      " 7229 7238 7250 7254 7260 7264 7272 7279 7302 7305 7313 7318 7328 7343\n",
      " 7351 7364 7367 7390 7396 7423 7440 7448 7449 7463 7464 7467 7473 7494\n",
      " 7497 7515 7531 7536 7537 7543 7545 7591 7593 7595 7602 7614 7617 7632\n",
      " 7636 7637 7641 7649 7650 7651 7668 7675 7676 7678 7687 7700 7712 7719\n",
      " 7725 7734 7735 7737 7745 7758 7759 7766 7781 7798 7828 7853 7854 7866\n",
      " 7871 7885 7890 7891 7900] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "n_features = [10, 50, 100, 200, 500, 1000, 2000, 5000, 7000]\n",
    "anova_results = pd.DataFrame()\n",
    "for n in n_features:\n",
    "    selector = [SelectKBest(f_classif, k=n)]\n",
    "    df = full_evaluation(train_data, train_labels, valid_data, valid_labels, selector, classifiers)\n",
    "    anova_results = pd.concat([anova_results, df])\n",
    "anova_results.to_csv('data2/anova.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = [10, 50, 100, 200, 500, 1000, 2000, 5000, 7000]\n",
    "chi2_results = pd.DataFrame()\n",
    "for n in n_features:\n",
    "    selector = [SelectKBest(chi2, k=n)]\n",
    "    df = full_evaluation(train_data, train_labels, valid_data, valid_labels, selector, classifiers)\n",
    "    chi2_results = pd.concat([chi2_results, df])\n",
    "chi2_results.to_csv('data2/chi2.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid + wrapper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boruta algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1 / 100\n",
      "Iteration: 2 / 100\n",
      "Iteration: 3 / 100\n",
      "Iteration: 4 / 100\n",
      "Iteration: 5 / 100\n",
      "Iteration: 6 / 100\n",
      "Iteration: 7 / 100\n",
      "Iteration: 8 / 100\n",
      "Iteration: 9 / 100\n",
      "Iteration: 10 / 100\n",
      "Iteration: 11 / 100\n",
      "Iteration: 12 / 100\n",
      "Iteration: 13 / 100\n",
      "Iteration: 14 / 100\n",
      "Iteration: 15 / 100\n",
      "Iteration: 16 / 100\n",
      "Iteration: 17 / 100\n",
      "Iteration: 18 / 100\n",
      "Iteration: 19 / 100\n",
      "Iteration: 20 / 100\n",
      "Iteration: 21 / 100\n",
      "Iteration: 22 / 100\n",
      "Iteration: 23 / 100\n",
      "Iteration: 24 / 100\n",
      "Iteration: 25 / 100\n",
      "Iteration: 26 / 100\n",
      "Iteration: 27 / 100\n",
      "Iteration: 28 / 100\n",
      "Iteration: 29 / 100\n",
      "Iteration: 30 / 100\n",
      "Iteration: 31 / 100\n",
      "Iteration: 32 / 100\n",
      "Iteration: 33 / 100\n",
      "Iteration: 34 / 100\n",
      "Iteration: 35 / 100\n",
      "Iteration: 36 / 100\n",
      "Iteration: 37 / 100\n",
      "Iteration: 38 / 100\n",
      "Iteration: 39 / 100\n",
      "Iteration: 40 / 100\n",
      "Iteration: 41 / 100\n",
      "Iteration: 42 / 100\n",
      "Iteration: 43 / 100\n",
      "Iteration: 44 / 100\n",
      "Iteration: 45 / 100\n",
      "Iteration: 46 / 100\n",
      "Iteration: 47 / 100\n",
      "Iteration: 48 / 100\n",
      "Iteration: 49 / 100\n",
      "Iteration: 50 / 100\n",
      "Iteration: 51 / 100\n",
      "Iteration: 52 / 100\n",
      "Iteration: 53 / 100\n",
      "Iteration: 54 / 100\n",
      "Iteration: 55 / 100\n",
      "Iteration: 56 / 100\n",
      "Iteration: 57 / 100\n",
      "Iteration: 58 / 100\n",
      "Iteration: 59 / 100\n",
      "Iteration: 60 / 100\n",
      "Iteration: 61 / 100\n",
      "Iteration: 62 / 100\n",
      "Iteration: 63 / 100\n",
      "Iteration: 64 / 100\n",
      "Iteration: 65 / 100\n",
      "Iteration: 66 / 100\n",
      "Iteration: 67 / 100\n",
      "Iteration: 68 / 100\n",
      "Iteration: 69 / 100\n",
      "Iteration: 70 / 100\n",
      "Iteration: 71 / 100\n",
      "Iteration: 72 / 100\n",
      "Iteration: 73 / 100\n",
      "Iteration: 74 / 100\n",
      "Iteration: 75 / 100\n",
      "Iteration: 76 / 100\n",
      "Iteration: 77 / 100\n",
      "Iteration: 78 / 100\n",
      "Iteration: 79 / 100\n",
      "Iteration: 80 / 100\n",
      "Iteration: 81 / 100\n",
      "Iteration: 82 / 100\n",
      "Iteration: 83 / 100\n",
      "Iteration: 84 / 100\n",
      "Iteration: 85 / 100\n",
      "Iteration: 86 / 100\n",
      "Iteration: 87 / 100\n",
      "Iteration: 88 / 100\n",
      "Iteration: 89 / 100\n",
      "Iteration: 90 / 100\n",
      "Iteration: 91 / 100\n",
      "Iteration: 92 / 100\n",
      "Iteration: 93 / 100\n",
      "Iteration: 94 / 100\n",
      "Iteration: 95 / 100\n",
      "Iteration: 96 / 100\n",
      "Iteration: 97 / 100\n",
      "Iteration: 98 / 100\n",
      "Iteration: 99 / 100\n",
      "\n",
      "\n",
      "BorutaPy finished running.\n",
      "\n",
      "Iteration: \t100 / 100\n",
      "Confirmed: \t165\n",
      "Tentative: \t0\n",
      "Rejected: \t7739\n"
     ]
    }
   ],
   "source": [
    "selector = [BorutaPy(estimator=RandomForestClassifier(n_estimators=100, max_depth=3), n_estimators='auto', verbose=1, random_state=0)]\n",
    "boruta_results = full_evaluation(train_data, train_labels, valid_data, valid_labels, selector, classifiers)\n",
    "boruta_results.to_csv('data/boruta.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 500 features.\n",
      "Fitting estimator with 499 features.\n",
      "Fitting estimator with 498 features.\n",
      "Fitting estimator with 497 features.\n",
      "Fitting estimator with 496 features.\n",
      "Fitting estimator with 495 features.\n",
      "Fitting estimator with 494 features.\n",
      "Fitting estimator with 493 features.\n",
      "Fitting estimator with 492 features.\n",
      "Fitting estimator with 491 features.\n",
      "Fitting estimator with 490 features.\n",
      "Fitting estimator with 489 features.\n",
      "Fitting estimator with 488 features.\n",
      "Fitting estimator with 487 features.\n",
      "Fitting estimator with 486 features.\n",
      "Fitting estimator with 485 features.\n",
      "Fitting estimator with 484 features.\n",
      "Fitting estimator with 483 features.\n",
      "Fitting estimator with 482 features.\n",
      "Fitting estimator with 481 features.\n",
      "Fitting estimator with 480 features.\n",
      "Fitting estimator with 479 features.\n",
      "Fitting estimator with 478 features.\n",
      "Fitting estimator with 477 features.\n",
      "Fitting estimator with 476 features.\n",
      "Fitting estimator with 475 features.\n",
      "Fitting estimator with 474 features.\n",
      "Fitting estimator with 473 features.\n",
      "Fitting estimator with 472 features.\n",
      "Fitting estimator with 471 features.\n",
      "Fitting estimator with 470 features.\n",
      "Fitting estimator with 469 features.\n",
      "Fitting estimator with 468 features.\n",
      "Fitting estimator with 467 features.\n",
      "Fitting estimator with 466 features.\n",
      "Fitting estimator with 465 features.\n",
      "Fitting estimator with 464 features.\n",
      "Fitting estimator with 463 features.\n",
      "Fitting estimator with 462 features.\n",
      "Fitting estimator with 461 features.\n",
      "Fitting estimator with 460 features.\n",
      "Fitting estimator with 459 features.\n",
      "Fitting estimator with 458 features.\n",
      "Fitting estimator with 457 features.\n",
      "Fitting estimator with 456 features.\n",
      "Fitting estimator with 455 features.\n",
      "Fitting estimator with 454 features.\n",
      "Fitting estimator with 453 features.\n",
      "Fitting estimator with 452 features.\n",
      "Fitting estimator with 451 features.\n",
      "Fitting estimator with 450 features.\n",
      "Fitting estimator with 449 features.\n",
      "Fitting estimator with 448 features.\n",
      "Fitting estimator with 447 features.\n",
      "Fitting estimator with 446 features.\n",
      "Fitting estimator with 445 features.\n",
      "Fitting estimator with 444 features.\n",
      "Fitting estimator with 443 features.\n",
      "Fitting estimator with 442 features.\n",
      "Fitting estimator with 441 features.\n",
      "Fitting estimator with 440 features.\n",
      "Fitting estimator with 439 features.\n",
      "Fitting estimator with 438 features.\n",
      "Fitting estimator with 437 features.\n",
      "Fitting estimator with 436 features.\n",
      "Fitting estimator with 435 features.\n",
      "Fitting estimator with 434 features.\n",
      "Fitting estimator with 433 features.\n",
      "Fitting estimator with 432 features.\n",
      "Fitting estimator with 431 features.\n",
      "Fitting estimator with 430 features.\n",
      "Fitting estimator with 429 features.\n",
      "Fitting estimator with 428 features.\n",
      "Fitting estimator with 427 features.\n",
      "Fitting estimator with 426 features.\n",
      "Fitting estimator with 425 features.\n",
      "Fitting estimator with 424 features.\n",
      "Fitting estimator with 423 features.\n",
      "Fitting estimator with 422 features.\n",
      "Fitting estimator with 421 features.\n",
      "Fitting estimator with 420 features.\n",
      "Fitting estimator with 419 features.\n",
      "Fitting estimator with 418 features.\n",
      "Fitting estimator with 417 features.\n",
      "Fitting estimator with 416 features.\n",
      "Fitting estimator with 415 features.\n",
      "Fitting estimator with 414 features.\n",
      "Fitting estimator with 413 features.\n",
      "Fitting estimator with 412 features.\n",
      "Fitting estimator with 411 features.\n",
      "Fitting estimator with 410 features.\n",
      "Fitting estimator with 409 features.\n",
      "Fitting estimator with 408 features.\n",
      "Fitting estimator with 407 features.\n",
      "Fitting estimator with 406 features.\n",
      "Fitting estimator with 405 features.\n",
      "Fitting estimator with 404 features.\n",
      "Fitting estimator with 403 features.\n",
      "Fitting estimator with 402 features.\n",
      "Fitting estimator with 401 features.\n",
      "Fitting estimator with 400 features.\n",
      "Fitting estimator with 399 features.\n",
      "Fitting estimator with 398 features.\n",
      "Fitting estimator with 397 features.\n",
      "Fitting estimator with 396 features.\n",
      "Fitting estimator with 395 features.\n",
      "Fitting estimator with 394 features.\n",
      "Fitting estimator with 393 features.\n",
      "Fitting estimator with 392 features.\n",
      "Fitting estimator with 391 features.\n",
      "Fitting estimator with 390 features.\n",
      "Fitting estimator with 389 features.\n",
      "Fitting estimator with 388 features.\n",
      "Fitting estimator with 387 features.\n",
      "Fitting estimator with 386 features.\n",
      "Fitting estimator with 385 features.\n",
      "Fitting estimator with 384 features.\n",
      "Fitting estimator with 383 features.\n",
      "Fitting estimator with 382 features.\n",
      "Fitting estimator with 381 features.\n",
      "Fitting estimator with 380 features.\n",
      "Fitting estimator with 379 features.\n",
      "Fitting estimator with 378 features.\n",
      "Fitting estimator with 377 features.\n",
      "Fitting estimator with 376 features.\n",
      "Fitting estimator with 375 features.\n",
      "Fitting estimator with 374 features.\n",
      "Fitting estimator with 373 features.\n",
      "Fitting estimator with 372 features.\n",
      "Fitting estimator with 371 features.\n",
      "Fitting estimator with 370 features.\n",
      "Fitting estimator with 369 features.\n",
      "Fitting estimator with 368 features.\n",
      "Fitting estimator with 367 features.\n",
      "Fitting estimator with 366 features.\n",
      "Fitting estimator with 365 features.\n",
      "Fitting estimator with 364 features.\n",
      "Fitting estimator with 363 features.\n",
      "Fitting estimator with 362 features.\n",
      "Fitting estimator with 361 features.\n",
      "Fitting estimator with 360 features.\n",
      "Fitting estimator with 359 features.\n",
      "Fitting estimator with 358 features.\n",
      "Fitting estimator with 357 features.\n",
      "Fitting estimator with 356 features.\n",
      "Fitting estimator with 355 features.\n",
      "Fitting estimator with 354 features.\n",
      "Fitting estimator with 353 features.\n",
      "Fitting estimator with 352 features.\n",
      "Fitting estimator with 351 features.\n",
      "Fitting estimator with 350 features.\n",
      "Fitting estimator with 349 features.\n",
      "Fitting estimator with 348 features.\n",
      "Fitting estimator with 347 features.\n",
      "Fitting estimator with 346 features.\n",
      "Fitting estimator with 345 features.\n",
      "Fitting estimator with 344 features.\n",
      "Fitting estimator with 343 features.\n",
      "Fitting estimator with 342 features.\n",
      "Fitting estimator with 341 features.\n",
      "Fitting estimator with 340 features.\n",
      "Fitting estimator with 339 features.\n",
      "Fitting estimator with 338 features.\n",
      "Fitting estimator with 337 features.\n",
      "Fitting estimator with 336 features.\n",
      "Fitting estimator with 335 features.\n",
      "Fitting estimator with 334 features.\n",
      "Fitting estimator with 333 features.\n",
      "Fitting estimator with 332 features.\n",
      "Fitting estimator with 331 features.\n",
      "Fitting estimator with 330 features.\n",
      "Fitting estimator with 329 features.\n",
      "Fitting estimator with 328 features.\n",
      "Fitting estimator with 327 features.\n",
      "Fitting estimator with 326 features.\n",
      "Fitting estimator with 325 features.\n",
      "Fitting estimator with 324 features.\n",
      "Fitting estimator with 323 features.\n",
      "Fitting estimator with 322 features.\n",
      "Fitting estimator with 321 features.\n",
      "Fitting estimator with 320 features.\n",
      "Fitting estimator with 319 features.\n",
      "Fitting estimator with 318 features.\n",
      "Fitting estimator with 317 features.\n",
      "Fitting estimator with 316 features.\n",
      "Fitting estimator with 315 features.\n",
      "Fitting estimator with 314 features.\n",
      "Fitting estimator with 313 features.\n",
      "Fitting estimator with 312 features.\n",
      "Fitting estimator with 311 features.\n",
      "Fitting estimator with 310 features.\n",
      "Fitting estimator with 309 features.\n",
      "Fitting estimator with 308 features.\n",
      "Fitting estimator with 307 features.\n",
      "Fitting estimator with 306 features.\n",
      "Fitting estimator with 305 features.\n",
      "Fitting estimator with 304 features.\n",
      "Fitting estimator with 303 features.\n",
      "Fitting estimator with 302 features.\n",
      "Fitting estimator with 301 features.\n",
      "Fitting estimator with 300 features.\n",
      "Fitting estimator with 299 features.\n",
      "Fitting estimator with 298 features.\n",
      "Fitting estimator with 297 features.\n",
      "Fitting estimator with 296 features.\n",
      "Fitting estimator with 295 features.\n",
      "Fitting estimator with 294 features.\n",
      "Fitting estimator with 293 features.\n",
      "Fitting estimator with 292 features.\n",
      "Fitting estimator with 291 features.\n",
      "Fitting estimator with 290 features.\n",
      "Fitting estimator with 289 features.\n",
      "Fitting estimator with 288 features.\n",
      "Fitting estimator with 287 features.\n",
      "Fitting estimator with 286 features.\n",
      "Fitting estimator with 285 features.\n",
      "Fitting estimator with 284 features.\n",
      "Fitting estimator with 283 features.\n",
      "Fitting estimator with 282 features.\n",
      "Fitting estimator with 281 features.\n",
      "Fitting estimator with 280 features.\n",
      "Fitting estimator with 279 features.\n",
      "Fitting estimator with 278 features.\n",
      "Fitting estimator with 277 features.\n",
      "Fitting estimator with 276 features.\n",
      "Fitting estimator with 275 features.\n",
      "Fitting estimator with 274 features.\n",
      "Fitting estimator with 273 features.\n",
      "Fitting estimator with 272 features.\n",
      "Fitting estimator with 271 features.\n",
      "Fitting estimator with 270 features.\n",
      "Fitting estimator with 269 features.\n",
      "Fitting estimator with 268 features.\n",
      "Fitting estimator with 267 features.\n",
      "Fitting estimator with 266 features.\n",
      "Fitting estimator with 265 features.\n",
      "Fitting estimator with 264 features.\n",
      "Fitting estimator with 263 features.\n",
      "Fitting estimator with 262 features.\n",
      "Fitting estimator with 261 features.\n",
      "Fitting estimator with 260 features.\n",
      "Fitting estimator with 259 features.\n",
      "Fitting estimator with 258 features.\n",
      "Fitting estimator with 257 features.\n",
      "Fitting estimator with 256 features.\n",
      "Fitting estimator with 255 features.\n",
      "Fitting estimator with 254 features.\n",
      "Fitting estimator with 253 features.\n",
      "Fitting estimator with 252 features.\n",
      "Fitting estimator with 251 features.\n",
      "Fitting estimator with 250 features.\n",
      "Fitting estimator with 249 features.\n",
      "Fitting estimator with 248 features.\n",
      "Fitting estimator with 247 features.\n",
      "Fitting estimator with 246 features.\n",
      "Fitting estimator with 245 features.\n",
      "Fitting estimator with 244 features.\n",
      "Fitting estimator with 243 features.\n",
      "Fitting estimator with 242 features.\n",
      "Fitting estimator with 241 features.\n",
      "Fitting estimator with 240 features.\n",
      "Fitting estimator with 239 features.\n",
      "Fitting estimator with 238 features.\n",
      "Fitting estimator with 237 features.\n",
      "Fitting estimator with 236 features.\n",
      "Fitting estimator with 235 features.\n",
      "Fitting estimator with 234 features.\n",
      "Fitting estimator with 233 features.\n",
      "Fitting estimator with 232 features.\n",
      "Fitting estimator with 231 features.\n",
      "Fitting estimator with 230 features.\n",
      "Fitting estimator with 229 features.\n",
      "Fitting estimator with 228 features.\n",
      "Fitting estimator with 227 features.\n",
      "Fitting estimator with 226 features.\n",
      "Fitting estimator with 225 features.\n",
      "Fitting estimator with 224 features.\n",
      "Fitting estimator with 223 features.\n",
      "Fitting estimator with 222 features.\n",
      "Fitting estimator with 221 features.\n",
      "Fitting estimator with 220 features.\n",
      "Fitting estimator with 219 features.\n",
      "Fitting estimator with 218 features.\n",
      "Fitting estimator with 217 features.\n",
      "Fitting estimator with 216 features.\n",
      "Fitting estimator with 215 features.\n",
      "Fitting estimator with 214 features.\n",
      "Fitting estimator with 213 features.\n",
      "Fitting estimator with 212 features.\n",
      "Fitting estimator with 211 features.\n",
      "Fitting estimator with 210 features.\n",
      "Fitting estimator with 209 features.\n",
      "Fitting estimator with 208 features.\n",
      "Fitting estimator with 207 features.\n",
      "Fitting estimator with 206 features.\n",
      "Fitting estimator with 205 features.\n",
      "Fitting estimator with 204 features.\n",
      "Fitting estimator with 203 features.\n",
      "Fitting estimator with 202 features.\n",
      "Fitting estimator with 201 features.\n",
      "Fitting estimator with 200 features.\n",
      "Fitting estimator with 199 features.\n",
      "Fitting estimator with 198 features.\n",
      "Fitting estimator with 197 features.\n",
      "Fitting estimator with 196 features.\n",
      "Fitting estimator with 195 features.\n",
      "Fitting estimator with 194 features.\n",
      "Fitting estimator with 193 features.\n",
      "Fitting estimator with 192 features.\n",
      "Fitting estimator with 191 features.\n",
      "Fitting estimator with 190 features.\n",
      "Fitting estimator with 189 features.\n",
      "Fitting estimator with 188 features.\n",
      "Fitting estimator with 187 features.\n",
      "Fitting estimator with 186 features.\n",
      "Fitting estimator with 185 features.\n",
      "Fitting estimator with 184 features.\n",
      "Fitting estimator with 183 features.\n",
      "Fitting estimator with 182 features.\n",
      "Fitting estimator with 181 features.\n",
      "Fitting estimator with 180 features.\n",
      "Fitting estimator with 179 features.\n",
      "Fitting estimator with 178 features.\n",
      "Fitting estimator with 177 features.\n",
      "Fitting estimator with 176 features.\n",
      "Fitting estimator with 175 features.\n",
      "Fitting estimator with 174 features.\n",
      "Fitting estimator with 173 features.\n",
      "Fitting estimator with 172 features.\n",
      "Fitting estimator with 171 features.\n",
      "Fitting estimator with 170 features.\n",
      "Fitting estimator with 169 features.\n",
      "Fitting estimator with 168 features.\n",
      "Fitting estimator with 167 features.\n",
      "Fitting estimator with 166 features.\n",
      "Fitting estimator with 165 features.\n",
      "Fitting estimator with 164 features.\n",
      "Fitting estimator with 163 features.\n",
      "Fitting estimator with 162 features.\n",
      "Fitting estimator with 161 features.\n",
      "Fitting estimator with 160 features.\n",
      "Fitting estimator with 159 features.\n",
      "Fitting estimator with 158 features.\n",
      "Fitting estimator with 157 features.\n",
      "Fitting estimator with 156 features.\n",
      "Fitting estimator with 155 features.\n",
      "Fitting estimator with 154 features.\n",
      "Fitting estimator with 153 features.\n",
      "Fitting estimator with 152 features.\n",
      "Fitting estimator with 151 features.\n",
      "Fitting estimator with 150 features.\n",
      "Fitting estimator with 149 features.\n",
      "Fitting estimator with 148 features.\n",
      "Fitting estimator with 147 features.\n",
      "Fitting estimator with 146 features.\n",
      "Fitting estimator with 145 features.\n",
      "Fitting estimator with 144 features.\n",
      "Fitting estimator with 143 features.\n",
      "Fitting estimator with 142 features.\n",
      "Fitting estimator with 141 features.\n",
      "Fitting estimator with 140 features.\n",
      "Fitting estimator with 139 features.\n",
      "Fitting estimator with 138 features.\n",
      "Fitting estimator with 137 features.\n",
      "Fitting estimator with 136 features.\n",
      "Fitting estimator with 135 features.\n",
      "Fitting estimator with 134 features.\n",
      "Fitting estimator with 133 features.\n",
      "Fitting estimator with 132 features.\n",
      "Fitting estimator with 131 features.\n",
      "Fitting estimator with 130 features.\n",
      "Fitting estimator with 129 features.\n",
      "Fitting estimator with 128 features.\n",
      "Fitting estimator with 127 features.\n",
      "Fitting estimator with 126 features.\n",
      "Fitting estimator with 125 features.\n",
      "Fitting estimator with 124 features.\n",
      "Fitting estimator with 123 features.\n",
      "Fitting estimator with 122 features.\n",
      "Fitting estimator with 121 features.\n",
      "Fitting estimator with 120 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 118 features.\n",
      "Fitting estimator with 117 features.\n",
      "Fitting estimator with 116 features.\n",
      "Fitting estimator with 115 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 113 features.\n",
      "Fitting estimator with 112 features.\n",
      "Fitting estimator with 111 features.\n",
      "Fitting estimator with 110 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 108 features.\n",
      "Fitting estimator with 107 features.\n",
      "Fitting estimator with 106 features.\n",
      "Fitting estimator with 105 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 103 features.\n",
      "Fitting estimator with 102 features.\n",
      "Fitting estimator with 101 features.\n"
     ]
    }
   ],
   "source": [
    "selector1 = RandomForestSelector(n_features=500)\n",
    "selector2 = RFE(estimator=RandomForestClassifier(n_estimators=100, max_depth=3), n_features_to_select=100, step=1, verbose=1)\n",
    "selectors = [[selector1, selector2]]\n",
    "stack_results = full_evaluation(train_data, train_labels, valid_data, valid_labels, selectors, classifiers)\n",
    "stack_results.to_csv('data/stack.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [  23   24   30   31   32   55   56   58   68   69   74   83   85   88\n",
      "   91  105  113  119  128  144  165  167  173  178  183  187  189  199\n",
      "  203  205  206  216  221  222  226  238  245  248  251  259  265  270\n",
      "  274  280  292  331  333  335  359  360  371  380  384  411  417  420\n",
      "  423  429  433  460  483  542  544  554  555  567  569  578  592  594\n",
      "  600  601  606  626  627  646  656  666  672  708  714  718  738  746\n",
      "  749  751  769  772  773  778  782  789  791  796  806  814  822  824\n",
      "  826  830  831  836  843  853  857  862  870  871  882  885  899  914\n",
      "  917  923  926  939  949  950  966  971  975  977  985  987  988  998\n",
      "  999 1012 1020 1026 1029 1030 1032 1040 1043 1044 1058 1065 1080 1086\n",
      " 1095 1104 1126 1129 1143 1166 1169 1176 1195 1206 1209 1216 1217 1219\n",
      " 1225 1226 1240 1241 1245 1248 1253 1255 1270 1279 1283 1306 1308 1338\n",
      " 1343 1349 1366 1397 1398 1420 1439 1445 1446 1478 1481 1489 1491 1493\n",
      " 1499 1512 1514 1515 1524 1527 1530 1532 1538 1551 1560 1563 1569 1579\n",
      " 1582 1591 1595 1621 1623 1633 1643 1656 1667 1679 1687 1688 1690 1702\n",
      " 1714 1723 1739 1771 1784 1786 1793 1797 1800 1801 1812 1813 1816 1838\n",
      " 1854 1863 1867 1868 1869 1873 1893 1914 1917 1920 1931 1934 1935 1945\n",
      " 1947 1967 1974 1988 2011 2020 2022 2043 2044 2045 2051 2053 2055 2063\n",
      " 2080 2088 2089 2100 2101 2112 2131 2145 2151 2155 2157 2159 2186 2187\n",
      " 2201 2203 2204 2206 2212 2214 2228 2230 2241 2244 2255 2267 2272 2273\n",
      " 2278 2287 2298 2323 2332 2345 2367 2382 2384 2388 2391 2399 2410 2432\n",
      " 2458 2463 2466 2467 2468 2472 2474 2480 2491 2495 2496 2498 2555 2562\n",
      " 2565 2588 2600 2609 2633 2643 2673 2689 2691 2693 2694 2697 2723 2727\n",
      " 2731 2733 2741 2743 2755 2762 2767 2773 2782 2785 2806 2808 2820 2825\n",
      " 2834 2850 2864 2877 2879 2887 2890 2895 2913 2914 2919 2926 2927 2929\n",
      " 2943 2952 2968 2969 2975 2999 3002 3007 3011 3016 3033 3035 3037 3039\n",
      " 3040 3041 3060 3078 3079 3083 3085 3087 3090 3092 3097 3122 3127 3137\n",
      " 3143 3172 3174 3186 3187 3189 3198 3203 3205 3219 3221 3230 3246 3255\n",
      " 3256 3263 3267 3276 3278 3280 3292 3295 3315 3327 3329 3333 3343 3355\n",
      " 3359 3360 3361 3384 3390 3393 3394 3415 3416 3420 3422 3427 3458 3474\n",
      " 3476 3493 3515 3525 3540 3564 3581 3589 3593 3596 3609 3614 3673 3696\n",
      " 3698 3701 3708 3715 3718 3723 3727 3731 3733 3748 3753 3769 3776 3788\n",
      " 3805 3822 3832 3839 3848 3860 3862 3863 3892 3906 3908 3926 3944 3978\n",
      " 3996 4001 4003 4015 4018 4034 4049 4063 4065 4069 4074 4082 4091 4111\n",
      " 4123 4125 4158 4169 4182 4191 4196 4209 4212 4216 4220 4223 4229 4251\n",
      " 4264 4274 4307 4338 4357 4364 4365 4367 4373 4379 4391 4448 4463 4465\n",
      " 4469 4471 4489 4506 4512 4528 4532 4537 4539 4541 4556 4576 4578 4581\n",
      " 4584 4599 4600 4603 4620 4623 4640 4641 4660 4662 4669 4679 4681 4688\n",
      " 4725 4730 4749 4754 4759 4762 4780 4781 4783 4784 4788 4791 4805 4813\n",
      " 4814 4840 4862 4871 4876 4878 4881 4915 4921 4930 4941 4977 4980 4984\n",
      " 4991 4995 5029 5031 5038 5060 5090 5115 5119 5121 5123 5138 5140 5148\n",
      " 5180 5194 5210 5212 5213 5214 5216 5218 5226 5230 5240 5245 5255 5266\n",
      " 5267 5283 5287 5289 5294 5296 5312 5322 5323 5326 5353 5355 5357 5362\n",
      " 5364 5383 5389 5398 5399 5406 5414 5415 5417 5427 5436 5449 5452 5459\n",
      " 5461 5470 5487 5499 5508 5511 5512 5526 5528 5530 5531 5535 5549 5557\n",
      " 5566 5568 5571 5579 5582 5586 5592 5609 5614 5635 5638 5645 5679 5680\n",
      " 5686 5688 5693 5703 5714 5718 5720 5733 5750 5761 5775 5782 5786 5789\n",
      " 5803 5806 5813 5816 5820 5822 5833 5845 5852 5854 5865 5866 5880 5898\n",
      " 5901 5943 5961 5963 5974 5976 5990 6009 6010 6028 6031 6032 6053 6061\n",
      " 6080 6086 6088 6097 6099 6118 6123 6125 6139 6158 6159 6170 6171 6175\n",
      " 6183 6186 6187 6196 6206 6208 6218 6221 6240 6242 6248 6258 6272 6280\n",
      " 6290 6299 6300 6323 6364 6372 6394 6415 6416 6420 6421 6424 6426 6434\n",
      " 6437 6447 6454 6460 6466 6470 6479 6495 6502 6517 6532 6535 6539 6546\n",
      " 6553 6556 6563 6571 6582 6583 6585 6605 6618 6622 6625 6626 6630 6657\n",
      " 6669 6670 6686 6703 6708 6714 6734 6751 6753 6757 6763 6792 6805 6807\n",
      " 6809 6823 6830 6831 6842 6843 6859 6867 6869 6896 6919 6932 6937 6945\n",
      " 6951 6959 6964 6971 6975 6978 6995 6997 7029 7033 7064 7090 7096 7112\n",
      " 7114 7140 7141 7148 7149 7163 7175 7176 7179 7191 7194 7196 7204 7219\n",
      " 7229 7238 7250 7254 7260 7264 7272 7279 7302 7305 7313 7318 7328 7343\n",
      " 7351 7364 7367 7390 7396 7423 7440 7448 7449 7463 7464 7467 7473 7494\n",
      " 7497 7515 7531 7536 7537 7543 7545 7591 7593 7595 7602 7614 7617 7632\n",
      " 7636 7637 7641 7649 7650 7651 7668 7675 7676 7678 7687 7700 7712 7719\n",
      " 7725 7734 7735 7737 7745 7758 7759 7766 7781 7798 7828 7853 7854 7866\n",
      " 7871 7885 7890 7891 7900] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:2853: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [  23   24   30   31   32   55   56   58   68   69   74   83   85   88\n",
      "   91  105  113  119  128  144  165  167  173  178  183  187  189  199\n",
      "  203  205  206  216  221  222  226  238  245  248  251  259  265  270\n",
      "  274  280  292  331  333  335  359  360  371  380  384  411  417  420\n",
      "  423  429  433  460  483  542  544  554  555  567  569  578  592  594\n",
      "  600  601  606  626  627  646  656  666  672  708  714  718  738  746\n",
      "  749  751  769  772  773  778  782  789  791  796  806  814  822  824\n",
      "  826  830  831  836  843  853  857  862  870  871  882  885  899  914\n",
      "  917  923  926  939  949  950  966  971  975  977  985  987  988  998\n",
      "  999 1012 1020 1026 1029 1030 1032 1040 1043 1044 1058 1065 1080 1086\n",
      " 1095 1104 1126 1129 1143 1166 1169 1176 1195 1206 1209 1216 1217 1219\n",
      " 1225 1226 1240 1241 1245 1248 1253 1255 1270 1279 1283 1306 1308 1338\n",
      " 1343 1349 1366 1397 1398 1420 1439 1445 1446 1478 1481 1489 1491 1493\n",
      " 1499 1512 1514 1515 1524 1527 1530 1532 1538 1551 1560 1563 1569 1579\n",
      " 1582 1591 1595 1621 1623 1633 1643 1656 1667 1679 1687 1688 1690 1702\n",
      " 1714 1723 1739 1771 1784 1786 1793 1797 1800 1801 1812 1813 1816 1838\n",
      " 1854 1863 1867 1868 1869 1873 1893 1914 1917 1920 1931 1934 1935 1945\n",
      " 1947 1967 1974 1988 2011 2020 2022 2043 2044 2045 2051 2053 2055 2063\n",
      " 2080 2088 2089 2100 2101 2112 2131 2145 2151 2155 2157 2159 2186 2187\n",
      " 2201 2203 2204 2206 2212 2214 2228 2230 2241 2244 2255 2267 2272 2273\n",
      " 2278 2287 2298 2323 2332 2345 2367 2382 2384 2388 2391 2399 2410 2432\n",
      " 2458 2463 2466 2467 2468 2472 2474 2480 2491 2495 2496 2498 2555 2562\n",
      " 2565 2588 2600 2609 2633 2643 2673 2689 2691 2693 2694 2697 2723 2727\n",
      " 2731 2733 2741 2743 2755 2762 2767 2773 2782 2785 2806 2808 2820 2825\n",
      " 2834 2850 2864 2877 2879 2887 2890 2895 2913 2914 2919 2926 2927 2929\n",
      " 2943 2952 2968 2969 2975 2999 3002 3007 3011 3016 3033 3035 3037 3039\n",
      " 3040 3041 3060 3078 3079 3083 3085 3087 3090 3092 3097 3122 3127 3137\n",
      " 3143 3172 3174 3186 3187 3189 3198 3203 3205 3219 3221 3230 3246 3255\n",
      " 3256 3263 3267 3276 3278 3280 3292 3295 3315 3327 3329 3333 3343 3355\n",
      " 3359 3360 3361 3384 3390 3393 3394 3415 3416 3420 3422 3427 3458 3474\n",
      " 3476 3493 3515 3525 3540 3564 3581 3589 3593 3596 3609 3614 3673 3696\n",
      " 3698 3701 3708 3715 3718 3723 3727 3731 3733 3748 3753 3769 3776 3788\n",
      " 3805 3822 3832 3839 3848 3860 3862 3863 3892 3906 3908 3926 3944 3978\n",
      " 3996 4001 4003 4015 4018 4034 4049 4063 4065 4069 4074 4082 4091 4111\n",
      " 4123 4125 4158 4169 4182 4191 4196 4209 4212 4216 4220 4223 4229 4251\n",
      " 4264 4274 4307 4338 4357 4364 4365 4367 4373 4379 4391 4448 4463 4465\n",
      " 4469 4471 4489 4506 4512 4528 4532 4537 4539 4541 4556 4576 4578 4581\n",
      " 4584 4599 4600 4603 4620 4623 4640 4641 4660 4662 4669 4679 4681 4688\n",
      " 4725 4730 4749 4754 4759 4762 4780 4781 4783 4784 4788 4791 4805 4813\n",
      " 4814 4840 4862 4871 4876 4878 4881 4915 4921 4930 4941 4977 4980 4984\n",
      " 4991 4995 5029 5031 5038 5060 5090 5115 5119 5121 5123 5138 5140 5148\n",
      " 5180 5194 5210 5212 5213 5214 5216 5218 5226 5230 5240 5245 5255 5266\n",
      " 5267 5283 5287 5289 5294 5296 5312 5322 5323 5326 5353 5355 5357 5362\n",
      " 5364 5383 5389 5398 5399 5406 5414 5415 5417 5427 5436 5449 5452 5459\n",
      " 5461 5470 5487 5499 5508 5511 5512 5526 5528 5530 5531 5535 5549 5557\n",
      " 5566 5568 5571 5579 5582 5586 5592 5609 5614 5635 5638 5645 5679 5680\n",
      " 5686 5688 5693 5703 5714 5718 5720 5733 5750 5761 5775 5782 5786 5789\n",
      " 5803 5806 5813 5816 5820 5822 5833 5845 5852 5854 5865 5866 5880 5898\n",
      " 5901 5943 5961 5963 5974 5976 5990 6009 6010 6028 6031 6032 6053 6061\n",
      " 6080 6086 6088 6097 6099 6118 6123 6125 6139 6158 6159 6170 6171 6175\n",
      " 6183 6186 6187 6196 6206 6208 6218 6221 6240 6242 6248 6258 6272 6280\n",
      " 6290 6299 6300 6323 6364 6372 6394 6415 6416 6420 6421 6424 6426 6434\n",
      " 6437 6447 6454 6460 6466 6470 6479 6495 6502 6517 6532 6535 6539 6546\n",
      " 6553 6556 6563 6571 6582 6583 6585 6605 6618 6622 6625 6626 6630 6657\n",
      " 6669 6670 6686 6703 6708 6714 6734 6751 6753 6757 6763 6792 6805 6807\n",
      " 6809 6823 6830 6831 6842 6843 6859 6867 6869 6896 6919 6932 6937 6945\n",
      " 6951 6959 6964 6971 6975 6978 6995 6997 7029 7033 7064 7090 7096 7112\n",
      " 7114 7140 7141 7148 7149 7163 7175 7176 7179 7191 7194 7196 7204 7219\n",
      " 7229 7238 7250 7254 7260 7264 7272 7279 7302 7305 7313 7318 7328 7343\n",
      " 7351 7364 7367 7390 7396 7423 7440 7448 7449 7463 7464 7467 7473 7494\n",
      " 7497 7515 7531 7536 7537 7543 7545 7591 7593 7595 7602 7614 7617 7632\n",
      " 7636 7637 7641 7649 7650 7651 7668 7675 7676 7678 7687 7700 7712 7719\n",
      " 7725 7734 7735 7737 7745 7758 7759 7766 7781 7798 7828 7853 7854 7866\n",
      " 7871 7885 7890 7891 7900] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:2853: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [  23   24   30   31   32   55   56   58   68   69   74   83   85   88\n",
      "   91  105  113  119  128  144  165  167  173  178  183  187  189  199\n",
      "  203  205  206  216  221  222  226  238  245  248  251  259  265  270\n",
      "  274  280  292  331  333  335  359  360  371  380  384  411  417  420\n",
      "  423  429  433  460  483  542  544  554  555  567  569  578  592  594\n",
      "  600  601  606  626  627  646  656  666  672  708  714  718  738  746\n",
      "  749  751  769  772  773  778  782  789  791  796  806  814  822  824\n",
      "  826  830  831  836  843  853  857  862  870  871  882  885  899  914\n",
      "  917  923  926  939  949  950  966  971  975  977  985  987  988  998\n",
      "  999 1012 1020 1026 1029 1030 1032 1040 1043 1044 1058 1065 1080 1086\n",
      " 1095 1104 1126 1129 1143 1166 1169 1176 1195 1206 1209 1216 1217 1219\n",
      " 1225 1226 1240 1241 1245 1248 1253 1255 1270 1279 1283 1306 1308 1338\n",
      " 1343 1349 1366 1397 1398 1420 1439 1445 1446 1478 1481 1489 1491 1493\n",
      " 1499 1512 1514 1515 1524 1527 1530 1532 1538 1551 1560 1563 1569 1579\n",
      " 1582 1591 1595 1621 1623 1633 1643 1656 1667 1679 1687 1688 1690 1702\n",
      " 1714 1723 1739 1771 1784 1786 1793 1797 1800 1801 1812 1813 1816 1838\n",
      " 1854 1863 1867 1868 1869 1873 1893 1914 1917 1920 1931 1934 1935 1945\n",
      " 1947 1967 1974 1988 2011 2020 2022 2043 2044 2045 2051 2053 2055 2063\n",
      " 2080 2088 2089 2100 2101 2112 2131 2145 2151 2155 2157 2159 2186 2187\n",
      " 2201 2203 2204 2206 2212 2214 2228 2230 2241 2244 2255 2267 2272 2273\n",
      " 2278 2287 2298 2323 2332 2345 2367 2382 2384 2388 2391 2399 2410 2432\n",
      " 2458 2463 2466 2467 2468 2472 2474 2480 2491 2495 2496 2498 2555 2562\n",
      " 2565 2588 2600 2609 2633 2643 2673 2689 2691 2693 2694 2697 2723 2727\n",
      " 2731 2733 2741 2743 2755 2762 2767 2773 2782 2785 2806 2808 2820 2825\n",
      " 2834 2850 2864 2877 2879 2887 2890 2895 2913 2914 2919 2926 2927 2929\n",
      " 2943 2952 2968 2969 2975 2999 3002 3007 3011 3016 3033 3035 3037 3039\n",
      " 3040 3041 3060 3078 3079 3083 3085 3087 3090 3092 3097 3122 3127 3137\n",
      " 3143 3172 3174 3186 3187 3189 3198 3203 3205 3219 3221 3230 3246 3255\n",
      " 3256 3263 3267 3276 3278 3280 3292 3295 3315 3327 3329 3333 3343 3355\n",
      " 3359 3360 3361 3384 3390 3393 3394 3415 3416 3420 3422 3427 3458 3474\n",
      " 3476 3493 3515 3525 3540 3564 3581 3589 3593 3596 3609 3614 3673 3696\n",
      " 3698 3701 3708 3715 3718 3723 3727 3731 3733 3748 3753 3769 3776 3788\n",
      " 3805 3822 3832 3839 3848 3860 3862 3863 3892 3906 3908 3926 3944 3978\n",
      " 3996 4001 4003 4015 4018 4034 4049 4063 4065 4069 4074 4082 4091 4111\n",
      " 4123 4125 4158 4169 4182 4191 4196 4209 4212 4216 4220 4223 4229 4251\n",
      " 4264 4274 4307 4338 4357 4364 4365 4367 4373 4379 4391 4448 4463 4465\n",
      " 4469 4471 4489 4506 4512 4528 4532 4537 4539 4541 4556 4576 4578 4581\n",
      " 4584 4599 4600 4603 4620 4623 4640 4641 4660 4662 4669 4679 4681 4688\n",
      " 4725 4730 4749 4754 4759 4762 4780 4781 4783 4784 4788 4791 4805 4813\n",
      " 4814 4840 4862 4871 4876 4878 4881 4915 4921 4930 4941 4977 4980 4984\n",
      " 4991 4995 5029 5031 5038 5060 5090 5115 5119 5121 5123 5138 5140 5148\n",
      " 5180 5194 5210 5212 5213 5214 5216 5218 5226 5230 5240 5245 5255 5266\n",
      " 5267 5283 5287 5289 5294 5296 5312 5322 5323 5326 5353 5355 5357 5362\n",
      " 5364 5383 5389 5398 5399 5406 5414 5415 5417 5427 5436 5449 5452 5459\n",
      " 5461 5470 5487 5499 5508 5511 5512 5526 5528 5530 5531 5535 5549 5557\n",
      " 5566 5568 5571 5579 5582 5586 5592 5609 5614 5635 5638 5645 5679 5680\n",
      " 5686 5688 5693 5703 5714 5718 5720 5733 5750 5761 5775 5782 5786 5789\n",
      " 5803 5806 5813 5816 5820 5822 5833 5845 5852 5854 5865 5866 5880 5898\n",
      " 5901 5943 5961 5963 5974 5976 5990 6009 6010 6028 6031 6032 6053 6061\n",
      " 6080 6086 6088 6097 6099 6118 6123 6125 6139 6158 6159 6170 6171 6175\n",
      " 6183 6186 6187 6196 6206 6208 6218 6221 6240 6242 6248 6258 6272 6280\n",
      " 6290 6299 6300 6323 6364 6372 6394 6415 6416 6420 6421 6424 6426 6434\n",
      " 6437 6447 6454 6460 6466 6470 6479 6495 6502 6517 6532 6535 6539 6546\n",
      " 6553 6556 6563 6571 6582 6583 6585 6605 6618 6622 6625 6626 6630 6657\n",
      " 6669 6670 6686 6703 6708 6714 6734 6751 6753 6757 6763 6792 6805 6807\n",
      " 6809 6823 6830 6831 6842 6843 6859 6867 6869 6896 6919 6932 6937 6945\n",
      " 6951 6959 6964 6971 6975 6978 6995 6997 7029 7033 7064 7090 7096 7112\n",
      " 7114 7140 7141 7148 7149 7163 7175 7176 7179 7191 7194 7196 7204 7219\n",
      " 7229 7238 7250 7254 7260 7264 7272 7279 7302 7305 7313 7318 7328 7343\n",
      " 7351 7364 7367 7390 7396 7423 7440 7448 7449 7463 7464 7467 7473 7494\n",
      " 7497 7515 7531 7536 7537 7543 7545 7591 7593 7595 7602 7614 7617 7632\n",
      " 7636 7637 7641 7649 7650 7651 7668 7675 7676 7678 7687 7700 7712 7719\n",
      " 7725 7734 7735 7737 7745 7758 7759 7766 7781 7798 7828 7853 7854 7866\n",
      " 7871 7885 7890 7891 7900] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:2853: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [  23   24   30   31   32   55   56   58   68   69   74   83   85   88\n",
      "   91  105  113  119  128  144  165  167  173  178  183  187  189  199\n",
      "  203  205  206  216  221  222  226  238  245  248  251  259  265  270\n",
      "  274  280  292  331  333  335  359  360  371  380  384  411  417  420\n",
      "  423  429  433  460  483  542  544  554  555  567  569  578  592  594\n",
      "  600  601  606  626  627  646  656  666  672  708  714  718  738  746\n",
      "  749  751  769  772  773  778  782  789  791  796  806  814  822  824\n",
      "  826  830  831  836  843  853  857  862  870  871  882  885  899  914\n",
      "  917  923  926  939  949  950  966  971  975  977  985  987  988  998\n",
      "  999 1012 1020 1026 1029 1030 1032 1040 1043 1044 1058 1065 1080 1086\n",
      " 1095 1104 1126 1129 1143 1166 1169 1176 1195 1206 1209 1216 1217 1219\n",
      " 1225 1226 1240 1241 1245 1248 1253 1255 1270 1279 1283 1306 1308 1338\n",
      " 1343 1349 1366 1397 1398 1420 1439 1445 1446 1478 1481 1489 1491 1493\n",
      " 1499 1512 1514 1515 1524 1527 1530 1532 1538 1551 1560 1563 1569 1579\n",
      " 1582 1591 1595 1621 1623 1633 1643 1656 1667 1679 1687 1688 1690 1702\n",
      " 1714 1723 1739 1771 1784 1786 1793 1797 1800 1801 1812 1813 1816 1838\n",
      " 1854 1863 1867 1868 1869 1873 1893 1914 1917 1920 1931 1934 1935 1945\n",
      " 1947 1967 1974 1988 2011 2020 2022 2043 2044 2045 2051 2053 2055 2063\n",
      " 2080 2088 2089 2100 2101 2112 2131 2145 2151 2155 2157 2159 2186 2187\n",
      " 2201 2203 2204 2206 2212 2214 2228 2230 2241 2244 2255 2267 2272 2273\n",
      " 2278 2287 2298 2323 2332 2345 2367 2382 2384 2388 2391 2399 2410 2432\n",
      " 2458 2463 2466 2467 2468 2472 2474 2480 2491 2495 2496 2498 2555 2562\n",
      " 2565 2588 2600 2609 2633 2643 2673 2689 2691 2693 2694 2697 2723 2727\n",
      " 2731 2733 2741 2743 2755 2762 2767 2773 2782 2785 2806 2808 2820 2825\n",
      " 2834 2850 2864 2877 2879 2887 2890 2895 2913 2914 2919 2926 2927 2929\n",
      " 2943 2952 2968 2969 2975 2999 3002 3007 3011 3016 3033 3035 3037 3039\n",
      " 3040 3041 3060 3078 3079 3083 3085 3087 3090 3092 3097 3122 3127 3137\n",
      " 3143 3172 3174 3186 3187 3189 3198 3203 3205 3219 3221 3230 3246 3255\n",
      " 3256 3263 3267 3276 3278 3280 3292 3295 3315 3327 3329 3333 3343 3355\n",
      " 3359 3360 3361 3384 3390 3393 3394 3415 3416 3420 3422 3427 3458 3474\n",
      " 3476 3493 3515 3525 3540 3564 3581 3589 3593 3596 3609 3614 3673 3696\n",
      " 3698 3701 3708 3715 3718 3723 3727 3731 3733 3748 3753 3769 3776 3788\n",
      " 3805 3822 3832 3839 3848 3860 3862 3863 3892 3906 3908 3926 3944 3978\n",
      " 3996 4001 4003 4015 4018 4034 4049 4063 4065 4069 4074 4082 4091 4111\n",
      " 4123 4125 4158 4169 4182 4191 4196 4209 4212 4216 4220 4223 4229 4251\n",
      " 4264 4274 4307 4338 4357 4364 4365 4367 4373 4379 4391 4448 4463 4465\n",
      " 4469 4471 4489 4506 4512 4528 4532 4537 4539 4541 4556 4576 4578 4581\n",
      " 4584 4599 4600 4603 4620 4623 4640 4641 4660 4662 4669 4679 4681 4688\n",
      " 4725 4730 4749 4754 4759 4762 4780 4781 4783 4784 4788 4791 4805 4813\n",
      " 4814 4840 4862 4871 4876 4878 4881 4915 4921 4930 4941 4977 4980 4984\n",
      " 4991 4995 5029 5031 5038 5060 5090 5115 5119 5121 5123 5138 5140 5148\n",
      " 5180 5194 5210 5212 5213 5214 5216 5218 5226 5230 5240 5245 5255 5266\n",
      " 5267 5283 5287 5289 5294 5296 5312 5322 5323 5326 5353 5355 5357 5362\n",
      " 5364 5383 5389 5398 5399 5406 5414 5415 5417 5427 5436 5449 5452 5459\n",
      " 5461 5470 5487 5499 5508 5511 5512 5526 5528 5530 5531 5535 5549 5557\n",
      " 5566 5568 5571 5579 5582 5586 5592 5609 5614 5635 5638 5645 5679 5680\n",
      " 5686 5688 5693 5703 5714 5718 5720 5733 5750 5761 5775 5782 5786 5789\n",
      " 5803 5806 5813 5816 5820 5822 5833 5845 5852 5854 5865 5866 5880 5898\n",
      " 5901 5943 5961 5963 5974 5976 5990 6009 6010 6028 6031 6032 6053 6061\n",
      " 6080 6086 6088 6097 6099 6118 6123 6125 6139 6158 6159 6170 6171 6175\n",
      " 6183 6186 6187 6196 6206 6208 6218 6221 6240 6242 6248 6258 6272 6280\n",
      " 6290 6299 6300 6323 6364 6372 6394 6415 6416 6420 6421 6424 6426 6434\n",
      " 6437 6447 6454 6460 6466 6470 6479 6495 6502 6517 6532 6535 6539 6546\n",
      " 6553 6556 6563 6571 6582 6583 6585 6605 6618 6622 6625 6626 6630 6657\n",
      " 6669 6670 6686 6703 6708 6714 6734 6751 6753 6757 6763 6792 6805 6807\n",
      " 6809 6823 6830 6831 6842 6843 6859 6867 6869 6896 6919 6932 6937 6945\n",
      " 6951 6959 6964 6971 6975 6978 6995 6997 7029 7033 7064 7090 7096 7112\n",
      " 7114 7140 7141 7148 7149 7163 7175 7176 7179 7191 7194 7196 7204 7219\n",
      " 7229 7238 7250 7254 7260 7264 7272 7279 7302 7305 7313 7318 7328 7343\n",
      " 7351 7364 7367 7390 7396 7423 7440 7448 7449 7463 7464 7467 7473 7494\n",
      " 7497 7515 7531 7536 7537 7543 7545 7591 7593 7595 7602 7614 7617 7632\n",
      " 7636 7637 7641 7649 7650 7651 7668 7675 7676 7678 7687 7700 7712 7719\n",
      " 7725 7734 7735 7737 7745 7758 7759 7766 7781 7798 7828 7853 7854 7866\n",
      " 7871 7885 7890 7891 7900] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:2853: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [  23   24   30   31   32   55   56   58   68   69   74   83   85   88\n",
      "   91  105  113  119  128  144  165  167  173  178  183  187  189  199\n",
      "  203  205  206  216  221  222  226  238  245  248  251  259  265  270\n",
      "  274  280  292  331  333  335  359  360  371  380  384  411  417  420\n",
      "  423  429  433  460  483  542  544  554  555  567  569  578  592  594\n",
      "  600  601  606  626  627  646  656  666  672  708  714  718  738  746\n",
      "  749  751  769  772  773  778  782  789  791  796  806  814  822  824\n",
      "  826  830  831  836  843  853  857  862  870  871  882  885  899  914\n",
      "  917  923  926  939  949  950  966  971  975  977  985  987  988  998\n",
      "  999 1012 1020 1026 1029 1030 1032 1040 1043 1044 1058 1065 1080 1086\n",
      " 1095 1104 1126 1129 1143 1166 1169 1176 1195 1206 1209 1216 1217 1219\n",
      " 1225 1226 1240 1241 1245 1248 1253 1255 1270 1279 1283 1306 1308 1338\n",
      " 1343 1349 1366 1397 1398 1420 1439 1445 1446 1478 1481 1489 1491 1493\n",
      " 1499 1512 1514 1515 1524 1527 1530 1532 1538 1551 1560 1563 1569 1579\n",
      " 1582 1591 1595 1621 1623 1633 1643 1656 1667 1679 1687 1688 1690 1702\n",
      " 1714 1723 1739 1771 1784 1786 1793 1797 1800 1801 1812 1813 1816 1838\n",
      " 1854 1863 1867 1868 1869 1873 1893 1914 1917 1920 1931 1934 1935 1945\n",
      " 1947 1967 1974 1988 2011 2020 2022 2043 2044 2045 2051 2053 2055 2063\n",
      " 2080 2088 2089 2100 2101 2112 2131 2145 2151 2155 2157 2159 2186 2187\n",
      " 2201 2203 2204 2206 2212 2214 2228 2230 2241 2244 2255 2267 2272 2273\n",
      " 2278 2287 2298 2323 2332 2345 2367 2382 2384 2388 2391 2399 2410 2432\n",
      " 2458 2463 2466 2467 2468 2472 2474 2480 2491 2495 2496 2498 2555 2562\n",
      " 2565 2588 2600 2609 2633 2643 2673 2689 2691 2693 2694 2697 2723 2727\n",
      " 2731 2733 2741 2743 2755 2762 2767 2773 2782 2785 2806 2808 2820 2825\n",
      " 2834 2850 2864 2877 2879 2887 2890 2895 2913 2914 2919 2926 2927 2929\n",
      " 2943 2952 2968 2969 2975 2999 3002 3007 3011 3016 3033 3035 3037 3039\n",
      " 3040 3041 3060 3078 3079 3083 3085 3087 3090 3092 3097 3122 3127 3137\n",
      " 3143 3172 3174 3186 3187 3189 3198 3203 3205 3219 3221 3230 3246 3255\n",
      " 3256 3263 3267 3276 3278 3280 3292 3295 3315 3327 3329 3333 3343 3355\n",
      " 3359 3360 3361 3384 3390 3393 3394 3415 3416 3420 3422 3427 3458 3474\n",
      " 3476 3493 3515 3525 3540 3564 3581 3589 3593 3596 3609 3614 3673 3696\n",
      " 3698 3701 3708 3715 3718 3723 3727 3731 3733 3748 3753 3769 3776 3788\n",
      " 3805 3822 3832 3839 3848 3860 3862 3863 3892 3906 3908 3926 3944 3978\n",
      " 3996 4001 4003 4015 4018 4034 4049 4063 4065 4069 4074 4082 4091 4111\n",
      " 4123 4125 4158 4169 4182 4191 4196 4209 4212 4216 4220 4223 4229 4251\n",
      " 4264 4274 4307 4338 4357 4364 4365 4367 4373 4379 4391 4448 4463 4465\n",
      " 4469 4471 4489 4506 4512 4528 4532 4537 4539 4541 4556 4576 4578 4581\n",
      " 4584 4599 4600 4603 4620 4623 4640 4641 4660 4662 4669 4679 4681 4688\n",
      " 4725 4730 4749 4754 4759 4762 4780 4781 4783 4784 4788 4791 4805 4813\n",
      " 4814 4840 4862 4871 4876 4878 4881 4915 4921 4930 4941 4977 4980 4984\n",
      " 4991 4995 5029 5031 5038 5060 5090 5115 5119 5121 5123 5138 5140 5148\n",
      " 5180 5194 5210 5212 5213 5214 5216 5218 5226 5230 5240 5245 5255 5266\n",
      " 5267 5283 5287 5289 5294 5296 5312 5322 5323 5326 5353 5355 5357 5362\n",
      " 5364 5383 5389 5398 5399 5406 5414 5415 5417 5427 5436 5449 5452 5459\n",
      " 5461 5470 5487 5499 5508 5511 5512 5526 5528 5530 5531 5535 5549 5557\n",
      " 5566 5568 5571 5579 5582 5586 5592 5609 5614 5635 5638 5645 5679 5680\n",
      " 5686 5688 5693 5703 5714 5718 5720 5733 5750 5761 5775 5782 5786 5789\n",
      " 5803 5806 5813 5816 5820 5822 5833 5845 5852 5854 5865 5866 5880 5898\n",
      " 5901 5943 5961 5963 5974 5976 5990 6009 6010 6028 6031 6032 6053 6061\n",
      " 6080 6086 6088 6097 6099 6118 6123 6125 6139 6158 6159 6170 6171 6175\n",
      " 6183 6186 6187 6196 6206 6208 6218 6221 6240 6242 6248 6258 6272 6280\n",
      " 6290 6299 6300 6323 6364 6372 6394 6415 6416 6420 6421 6424 6426 6434\n",
      " 6437 6447 6454 6460 6466 6470 6479 6495 6502 6517 6532 6535 6539 6546\n",
      " 6553 6556 6563 6571 6582 6583 6585 6605 6618 6622 6625 6626 6630 6657\n",
      " 6669 6670 6686 6703 6708 6714 6734 6751 6753 6757 6763 6792 6805 6807\n",
      " 6809 6823 6830 6831 6842 6843 6859 6867 6869 6896 6919 6932 6937 6945\n",
      " 6951 6959 6964 6971 6975 6978 6995 6997 7029 7033 7064 7090 7096 7112\n",
      " 7114 7140 7141 7148 7149 7163 7175 7176 7179 7191 7194 7196 7204 7219\n",
      " 7229 7238 7250 7254 7260 7264 7272 7279 7302 7305 7313 7318 7328 7343\n",
      " 7351 7364 7367 7390 7396 7423 7440 7448 7449 7463 7464 7467 7473 7494\n",
      " 7497 7515 7531 7536 7537 7543 7545 7591 7593 7595 7602 7614 7617 7632\n",
      " 7636 7637 7641 7649 7650 7651 7668 7675 7676 7678 7687 7700 7712 7719\n",
      " 7725 7734 7735 7737 7745 7758 7759 7766 7781 7798 7828 7853 7854 7866\n",
      " 7871 7885 7890 7891 7900] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:2853: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [  23   24   30   31   32   55   56   58   68   69   74   83   85   88\n",
      "   91  105  113  119  128  144  165  167  173  178  183  187  189  199\n",
      "  203  205  206  216  221  222  226  238  245  248  251  259  265  270\n",
      "  274  280  292  331  333  335  359  360  371  380  384  411  417  420\n",
      "  423  429  433  460  483  542  544  554  555  567  569  578  592  594\n",
      "  600  601  606  626  627  646  656  666  672  708  714  718  738  746\n",
      "  749  751  769  772  773  778  782  789  791  796  806  814  822  824\n",
      "  826  830  831  836  843  853  857  862  870  871  882  885  899  914\n",
      "  917  923  926  939  949  950  966  971  975  977  985  987  988  998\n",
      "  999 1012 1020 1026 1029 1030 1032 1040 1043 1044 1058 1065 1080 1086\n",
      " 1095 1104 1126 1129 1143 1166 1169 1176 1195 1206 1209 1216 1217 1219\n",
      " 1225 1226 1240 1241 1245 1248 1253 1255 1270 1279 1283 1306 1308 1338\n",
      " 1343 1349 1366 1397 1398 1420 1439 1445 1446 1478 1481 1489 1491 1493\n",
      " 1499 1512 1514 1515 1524 1527 1530 1532 1538 1551 1560 1563 1569 1579\n",
      " 1582 1591 1595 1621 1623 1633 1643 1656 1667 1679 1687 1688 1690 1702\n",
      " 1714 1723 1739 1771 1784 1786 1793 1797 1800 1801 1812 1813 1816 1838\n",
      " 1854 1863 1867 1868 1869 1873 1893 1914 1917 1920 1931 1934 1935 1945\n",
      " 1947 1967 1974 1988 2011 2020 2022 2043 2044 2045 2051 2053 2055 2063\n",
      " 2080 2088 2089 2100 2101 2112 2131 2145 2151 2155 2157 2159 2186 2187\n",
      " 2201 2203 2204 2206 2212 2214 2228 2230 2241 2244 2255 2267 2272 2273\n",
      " 2278 2287 2298 2323 2332 2345 2367 2382 2384 2388 2391 2399 2410 2432\n",
      " 2458 2463 2466 2467 2468 2472 2474 2480 2491 2495 2496 2498 2555 2562\n",
      " 2565 2588 2600 2609 2633 2643 2673 2689 2691 2693 2694 2697 2723 2727\n",
      " 2731 2733 2741 2743 2755 2762 2767 2773 2782 2785 2806 2808 2820 2825\n",
      " 2834 2850 2864 2877 2879 2887 2890 2895 2913 2914 2919 2926 2927 2929\n",
      " 2943 2952 2968 2969 2975 2999 3002 3007 3011 3016 3033 3035 3037 3039\n",
      " 3040 3041 3060 3078 3079 3083 3085 3087 3090 3092 3097 3122 3127 3137\n",
      " 3143 3172 3174 3186 3187 3189 3198 3203 3205 3219 3221 3230 3246 3255\n",
      " 3256 3263 3267 3276 3278 3280 3292 3295 3315 3327 3329 3333 3343 3355\n",
      " 3359 3360 3361 3384 3390 3393 3394 3415 3416 3420 3422 3427 3458 3474\n",
      " 3476 3493 3515 3525 3540 3564 3581 3589 3593 3596 3609 3614 3673 3696\n",
      " 3698 3701 3708 3715 3718 3723 3727 3731 3733 3748 3753 3769 3776 3788\n",
      " 3805 3822 3832 3839 3848 3860 3862 3863 3892 3906 3908 3926 3944 3978\n",
      " 3996 4001 4003 4015 4018 4034 4049 4063 4065 4069 4074 4082 4091 4111\n",
      " 4123 4125 4158 4169 4182 4191 4196 4209 4212 4216 4220 4223 4229 4251\n",
      " 4264 4274 4307 4338 4357 4364 4365 4367 4373 4379 4391 4448 4463 4465\n",
      " 4469 4471 4489 4506 4512 4528 4532 4537 4539 4541 4556 4576 4578 4581\n",
      " 4584 4599 4600 4603 4620 4623 4640 4641 4660 4662 4669 4679 4681 4688\n",
      " 4725 4730 4749 4754 4759 4762 4780 4781 4783 4784 4788 4791 4805 4813\n",
      " 4814 4840 4862 4871 4876 4878 4881 4915 4921 4930 4941 4977 4980 4984\n",
      " 4991 4995 5029 5031 5038 5060 5090 5115 5119 5121 5123 5138 5140 5148\n",
      " 5180 5194 5210 5212 5213 5214 5216 5218 5226 5230 5240 5245 5255 5266\n",
      " 5267 5283 5287 5289 5294 5296 5312 5322 5323 5326 5353 5355 5357 5362\n",
      " 5364 5383 5389 5398 5399 5406 5414 5415 5417 5427 5436 5449 5452 5459\n",
      " 5461 5470 5487 5499 5508 5511 5512 5526 5528 5530 5531 5535 5549 5557\n",
      " 5566 5568 5571 5579 5582 5586 5592 5609 5614 5635 5638 5645 5679 5680\n",
      " 5686 5688 5693 5703 5714 5718 5720 5733 5750 5761 5775 5782 5786 5789\n",
      " 5803 5806 5813 5816 5820 5822 5833 5845 5852 5854 5865 5866 5880 5898\n",
      " 5901 5943 5961 5963 5974 5976 5990 6009 6010 6028 6031 6032 6053 6061\n",
      " 6080 6086 6088 6097 6099 6118 6123 6125 6139 6158 6159 6170 6171 6175\n",
      " 6183 6186 6187 6196 6206 6208 6218 6221 6240 6242 6248 6258 6272 6280\n",
      " 6290 6299 6300 6323 6364 6372 6394 6415 6416 6420 6421 6424 6426 6434\n",
      " 6437 6447 6454 6460 6466 6470 6479 6495 6502 6517 6532 6535 6539 6546\n",
      " 6553 6556 6563 6571 6582 6583 6585 6605 6618 6622 6625 6626 6630 6657\n",
      " 6669 6670 6686 6703 6708 6714 6734 6751 6753 6757 6763 6792 6805 6807\n",
      " 6809 6823 6830 6831 6842 6843 6859 6867 6869 6896 6919 6932 6937 6945\n",
      " 6951 6959 6964 6971 6975 6978 6995 6997 7029 7033 7064 7090 7096 7112\n",
      " 7114 7140 7141 7148 7149 7163 7175 7176 7179 7191 7194 7196 7204 7219\n",
      " 7229 7238 7250 7254 7260 7264 7272 7279 7302 7305 7313 7318 7328 7343\n",
      " 7351 7364 7367 7390 7396 7423 7440 7448 7449 7463 7464 7467 7473 7494\n",
      " 7497 7515 7531 7536 7537 7543 7545 7591 7593 7595 7602 7614 7617 7632\n",
      " 7636 7637 7641 7649 7650 7651 7668 7675 7676 7678 7687 7700 7712 7719\n",
      " 7725 7734 7735 7737 7745 7758 7759 7766 7781 7798 7828 7853 7854 7866\n",
      " 7871 7885 7890 7891 7900] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:2853: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [  23   24   30   31   32   55   56   58   68   69   74   83   85   88\n",
      "   91  105  113  119  128  144  165  167  173  178  183  187  189  199\n",
      "  203  205  206  216  221  222  226  238  245  248  251  259  265  270\n",
      "  274  280  292  331  333  335  359  360  371  380  384  411  417  420\n",
      "  423  429  433  460  483  542  544  554  555  567  569  578  592  594\n",
      "  600  601  606  626  627  646  656  666  672  708  714  718  738  746\n",
      "  749  751  769  772  773  778  782  789  791  796  806  814  822  824\n",
      "  826  830  831  836  843  853  857  862  870  871  882  885  899  914\n",
      "  917  923  926  939  949  950  966  971  975  977  985  987  988  998\n",
      "  999 1012 1020 1026 1029 1030 1032 1040 1043 1044 1058 1065 1080 1086\n",
      " 1095 1104 1126 1129 1143 1166 1169 1176 1195 1206 1209 1216 1217 1219\n",
      " 1225 1226 1240 1241 1245 1248 1253 1255 1270 1279 1283 1306 1308 1338\n",
      " 1343 1349 1366 1397 1398 1420 1439 1445 1446 1478 1481 1489 1491 1493\n",
      " 1499 1512 1514 1515 1524 1527 1530 1532 1538 1551 1560 1563 1569 1579\n",
      " 1582 1591 1595 1621 1623 1633 1643 1656 1667 1679 1687 1688 1690 1702\n",
      " 1714 1723 1739 1771 1784 1786 1793 1797 1800 1801 1812 1813 1816 1838\n",
      " 1854 1863 1867 1868 1869 1873 1893 1914 1917 1920 1931 1934 1935 1945\n",
      " 1947 1967 1974 1988 2011 2020 2022 2043 2044 2045 2051 2053 2055 2063\n",
      " 2080 2088 2089 2100 2101 2112 2131 2145 2151 2155 2157 2159 2186 2187\n",
      " 2201 2203 2204 2206 2212 2214 2228 2230 2241 2244 2255 2267 2272 2273\n",
      " 2278 2287 2298 2323 2332 2345 2367 2382 2384 2388 2391 2399 2410 2432\n",
      " 2458 2463 2466 2467 2468 2472 2474 2480 2491 2495 2496 2498 2555 2562\n",
      " 2565 2588 2600 2609 2633 2643 2673 2689 2691 2693 2694 2697 2723 2727\n",
      " 2731 2733 2741 2743 2755 2762 2767 2773 2782 2785 2806 2808 2820 2825\n",
      " 2834 2850 2864 2877 2879 2887 2890 2895 2913 2914 2919 2926 2927 2929\n",
      " 2943 2952 2968 2969 2975 2999 3002 3007 3011 3016 3033 3035 3037 3039\n",
      " 3040 3041 3060 3078 3079 3083 3085 3087 3090 3092 3097 3122 3127 3137\n",
      " 3143 3172 3174 3186 3187 3189 3198 3203 3205 3219 3221 3230 3246 3255\n",
      " 3256 3263 3267 3276 3278 3280 3292 3295 3315 3327 3329 3333 3343 3355\n",
      " 3359 3360 3361 3384 3390 3393 3394 3415 3416 3420 3422 3427 3458 3474\n",
      " 3476 3493 3515 3525 3540 3564 3581 3589 3593 3596 3609 3614 3673 3696\n",
      " 3698 3701 3708 3715 3718 3723 3727 3731 3733 3748 3753 3769 3776 3788\n",
      " 3805 3822 3832 3839 3848 3860 3862 3863 3892 3906 3908 3926 3944 3978\n",
      " 3996 4001 4003 4015 4018 4034 4049 4063 4065 4069 4074 4082 4091 4111\n",
      " 4123 4125 4158 4169 4182 4191 4196 4209 4212 4216 4220 4223 4229 4251\n",
      " 4264 4274 4307 4338 4357 4364 4365 4367 4373 4379 4391 4448 4463 4465\n",
      " 4469 4471 4489 4506 4512 4528 4532 4537 4539 4541 4556 4576 4578 4581\n",
      " 4584 4599 4600 4603 4620 4623 4640 4641 4660 4662 4669 4679 4681 4688\n",
      " 4725 4730 4749 4754 4759 4762 4780 4781 4783 4784 4788 4791 4805 4813\n",
      " 4814 4840 4862 4871 4876 4878 4881 4915 4921 4930 4941 4977 4980 4984\n",
      " 4991 4995 5029 5031 5038 5060 5090 5115 5119 5121 5123 5138 5140 5148\n",
      " 5180 5194 5210 5212 5213 5214 5216 5218 5226 5230 5240 5245 5255 5266\n",
      " 5267 5283 5287 5289 5294 5296 5312 5322 5323 5326 5353 5355 5357 5362\n",
      " 5364 5383 5389 5398 5399 5406 5414 5415 5417 5427 5436 5449 5452 5459\n",
      " 5461 5470 5487 5499 5508 5511 5512 5526 5528 5530 5531 5535 5549 5557\n",
      " 5566 5568 5571 5579 5582 5586 5592 5609 5614 5635 5638 5645 5679 5680\n",
      " 5686 5688 5693 5703 5714 5718 5720 5733 5750 5761 5775 5782 5786 5789\n",
      " 5803 5806 5813 5816 5820 5822 5833 5845 5852 5854 5865 5866 5880 5898\n",
      " 5901 5943 5961 5963 5974 5976 5990 6009 6010 6028 6031 6032 6053 6061\n",
      " 6080 6086 6088 6097 6099 6118 6123 6125 6139 6158 6159 6170 6171 6175\n",
      " 6183 6186 6187 6196 6206 6208 6218 6221 6240 6242 6248 6258 6272 6280\n",
      " 6290 6299 6300 6323 6364 6372 6394 6415 6416 6420 6421 6424 6426 6434\n",
      " 6437 6447 6454 6460 6466 6470 6479 6495 6502 6517 6532 6535 6539 6546\n",
      " 6553 6556 6563 6571 6582 6583 6585 6605 6618 6622 6625 6626 6630 6657\n",
      " 6669 6670 6686 6703 6708 6714 6734 6751 6753 6757 6763 6792 6805 6807\n",
      " 6809 6823 6830 6831 6842 6843 6859 6867 6869 6896 6919 6932 6937 6945\n",
      " 6951 6959 6964 6971 6975 6978 6995 6997 7029 7033 7064 7090 7096 7112\n",
      " 7114 7140 7141 7148 7149 7163 7175 7176 7179 7191 7194 7196 7204 7219\n",
      " 7229 7238 7250 7254 7260 7264 7272 7279 7302 7305 7313 7318 7328 7343\n",
      " 7351 7364 7367 7390 7396 7423 7440 7448 7449 7463 7464 7467 7473 7494\n",
      " 7497 7515 7531 7536 7537 7543 7545 7591 7593 7595 7602 7614 7617 7632\n",
      " 7636 7637 7641 7649 7650 7651 7668 7675 7676 7678 7687 7700 7712 7719\n",
      " 7725 7734 7735 7737 7745 7758 7759 7766 7781 7798 7828 7853 7854 7866\n",
      " 7871 7885 7890 7891 7900] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:2853: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [  23   24   30   31   32   55   56   58   68   69   74   83   85   88\n",
      "   91  105  113  119  128  144  165  167  173  178  183  187  189  199\n",
      "  203  205  206  216  221  222  226  238  245  248  251  259  265  270\n",
      "  274  280  292  331  333  335  359  360  371  380  384  411  417  420\n",
      "  423  429  433  460  483  542  544  554  555  567  569  578  592  594\n",
      "  600  601  606  626  627  646  656  666  672  708  714  718  738  746\n",
      "  749  751  769  772  773  778  782  789  791  796  806  814  822  824\n",
      "  826  830  831  836  843  853  857  862  870  871  882  885  899  914\n",
      "  917  923  926  939  949  950  966  971  975  977  985  987  988  998\n",
      "  999 1012 1020 1026 1029 1030 1032 1040 1043 1044 1058 1065 1080 1086\n",
      " 1095 1104 1126 1129 1143 1166 1169 1176 1195 1206 1209 1216 1217 1219\n",
      " 1225 1226 1240 1241 1245 1248 1253 1255 1270 1279 1283 1306 1308 1338\n",
      " 1343 1349 1366 1397 1398 1420 1439 1445 1446 1478 1481 1489 1491 1493\n",
      " 1499 1512 1514 1515 1524 1527 1530 1532 1538 1551 1560 1563 1569 1579\n",
      " 1582 1591 1595 1621 1623 1633 1643 1656 1667 1679 1687 1688 1690 1702\n",
      " 1714 1723 1739 1771 1784 1786 1793 1797 1800 1801 1812 1813 1816 1838\n",
      " 1854 1863 1867 1868 1869 1873 1893 1914 1917 1920 1931 1934 1935 1945\n",
      " 1947 1967 1974 1988 2011 2020 2022 2043 2044 2045 2051 2053 2055 2063\n",
      " 2080 2088 2089 2100 2101 2112 2131 2145 2151 2155 2157 2159 2186 2187\n",
      " 2201 2203 2204 2206 2212 2214 2228 2230 2241 2244 2255 2267 2272 2273\n",
      " 2278 2287 2298 2323 2332 2345 2367 2382 2384 2388 2391 2399 2410 2432\n",
      " 2458 2463 2466 2467 2468 2472 2474 2480 2491 2495 2496 2498 2555 2562\n",
      " 2565 2588 2600 2609 2633 2643 2673 2689 2691 2693 2694 2697 2723 2727\n",
      " 2731 2733 2741 2743 2755 2762 2767 2773 2782 2785 2806 2808 2820 2825\n",
      " 2834 2850 2864 2877 2879 2887 2890 2895 2913 2914 2919 2926 2927 2929\n",
      " 2943 2952 2968 2969 2975 2999 3002 3007 3011 3016 3033 3035 3037 3039\n",
      " 3040 3041 3060 3078 3079 3083 3085 3087 3090 3092 3097 3122 3127 3137\n",
      " 3143 3172 3174 3186 3187 3189 3198 3203 3205 3219 3221 3230 3246 3255\n",
      " 3256 3263 3267 3276 3278 3280 3292 3295 3315 3327 3329 3333 3343 3355\n",
      " 3359 3360 3361 3384 3390 3393 3394 3415 3416 3420 3422 3427 3458 3474\n",
      " 3476 3493 3515 3525 3540 3564 3581 3589 3593 3596 3609 3614 3673 3696\n",
      " 3698 3701 3708 3715 3718 3723 3727 3731 3733 3748 3753 3769 3776 3788\n",
      " 3805 3822 3832 3839 3848 3860 3862 3863 3892 3906 3908 3926 3944 3978\n",
      " 3996 4001 4003 4015 4018 4034 4049 4063 4065 4069 4074 4082 4091 4111\n",
      " 4123 4125 4158 4169 4182 4191 4196 4209 4212 4216 4220 4223 4229 4251\n",
      " 4264 4274 4307 4338 4357 4364 4365 4367 4373 4379 4391 4448 4463 4465\n",
      " 4469 4471 4489 4506 4512 4528 4532 4537 4539 4541 4556 4576 4578 4581\n",
      " 4584 4599 4600 4603 4620 4623 4640 4641 4660 4662 4669 4679 4681 4688\n",
      " 4725 4730 4749 4754 4759 4762 4780 4781 4783 4784 4788 4791 4805 4813\n",
      " 4814 4840 4862 4871 4876 4878 4881 4915 4921 4930 4941 4977 4980 4984\n",
      " 4991 4995 5029 5031 5038 5060 5090 5115 5119 5121 5123 5138 5140 5148\n",
      " 5180 5194 5210 5212 5213 5214 5216 5218 5226 5230 5240 5245 5255 5266\n",
      " 5267 5283 5287 5289 5294 5296 5312 5322 5323 5326 5353 5355 5357 5362\n",
      " 5364 5383 5389 5398 5399 5406 5414 5415 5417 5427 5436 5449 5452 5459\n",
      " 5461 5470 5487 5499 5508 5511 5512 5526 5528 5530 5531 5535 5549 5557\n",
      " 5566 5568 5571 5579 5582 5586 5592 5609 5614 5635 5638 5645 5679 5680\n",
      " 5686 5688 5693 5703 5714 5718 5720 5733 5750 5761 5775 5782 5786 5789\n",
      " 5803 5806 5813 5816 5820 5822 5833 5845 5852 5854 5865 5866 5880 5898\n",
      " 5901 5943 5961 5963 5974 5976 5990 6009 6010 6028 6031 6032 6053 6061\n",
      " 6080 6086 6088 6097 6099 6118 6123 6125 6139 6158 6159 6170 6171 6175\n",
      " 6183 6186 6187 6196 6206 6208 6218 6221 6240 6242 6248 6258 6272 6280\n",
      " 6290 6299 6300 6323 6364 6372 6394 6415 6416 6420 6421 6424 6426 6434\n",
      " 6437 6447 6454 6460 6466 6470 6479 6495 6502 6517 6532 6535 6539 6546\n",
      " 6553 6556 6563 6571 6582 6583 6585 6605 6618 6622 6625 6626 6630 6657\n",
      " 6669 6670 6686 6703 6708 6714 6734 6751 6753 6757 6763 6792 6805 6807\n",
      " 6809 6823 6830 6831 6842 6843 6859 6867 6869 6896 6919 6932 6937 6945\n",
      " 6951 6959 6964 6971 6975 6978 6995 6997 7029 7033 7064 7090 7096 7112\n",
      " 7114 7140 7141 7148 7149 7163 7175 7176 7179 7191 7194 7196 7204 7219\n",
      " 7229 7238 7250 7254 7260 7264 7272 7279 7302 7305 7313 7318 7328 7343\n",
      " 7351 7364 7367 7390 7396 7423 7440 7448 7449 7463 7464 7467 7473 7494\n",
      " 7497 7515 7531 7536 7537 7543 7545 7591 7593 7595 7602 7614 7617 7632\n",
      " 7636 7637 7641 7649 7650 7651 7668 7675 7676 7678 7687 7700 7712 7719\n",
      " 7725 7734 7735 7737 7745 7758 7759 7766 7781 7798 7828 7853 7854 7866\n",
      " 7871 7885 7890 7891 7900] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:2853: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [  23   24   30   31   32   55   56   58   68   69   74   83   85   88\n",
      "   91  105  113  119  128  144  165  167  173  178  183  187  189  199\n",
      "  203  205  206  216  221  222  226  238  245  248  251  259  265  270\n",
      "  274  280  292  331  333  335  359  360  371  380  384  411  417  420\n",
      "  423  429  433  460  483  542  544  554  555  567  569  578  592  594\n",
      "  600  601  606  626  627  646  656  666  672  708  714  718  738  746\n",
      "  749  751  769  772  773  778  782  789  791  796  806  814  822  824\n",
      "  826  830  831  836  843  853  857  862  870  871  882  885  899  914\n",
      "  917  923  926  939  949  950  966  971  975  977  985  987  988  998\n",
      "  999 1012 1020 1026 1029 1030 1032 1040 1043 1044 1058 1065 1080 1086\n",
      " 1095 1104 1126 1129 1143 1166 1169 1176 1195 1206 1209 1216 1217 1219\n",
      " 1225 1226 1240 1241 1245 1248 1253 1255 1270 1279 1283 1306 1308 1338\n",
      " 1343 1349 1366 1397 1398 1420 1439 1445 1446 1478 1481 1489 1491 1493\n",
      " 1499 1512 1514 1515 1524 1527 1530 1532 1538 1551 1560 1563 1569 1579\n",
      " 1582 1591 1595 1621 1623 1633 1643 1656 1667 1679 1687 1688 1690 1702\n",
      " 1714 1723 1739 1771 1784 1786 1793 1797 1800 1801 1812 1813 1816 1838\n",
      " 1854 1863 1867 1868 1869 1873 1893 1914 1917 1920 1931 1934 1935 1945\n",
      " 1947 1967 1974 1988 2011 2020 2022 2043 2044 2045 2051 2053 2055 2063\n",
      " 2080 2088 2089 2100 2101 2112 2131 2145 2151 2155 2157 2159 2186 2187\n",
      " 2201 2203 2204 2206 2212 2214 2228 2230 2241 2244 2255 2267 2272 2273\n",
      " 2278 2287 2298 2323 2332 2345 2367 2382 2384 2388 2391 2399 2410 2432\n",
      " 2458 2463 2466 2467 2468 2472 2474 2480 2491 2495 2496 2498 2555 2562\n",
      " 2565 2588 2600 2609 2633 2643 2673 2689 2691 2693 2694 2697 2723 2727\n",
      " 2731 2733 2741 2743 2755 2762 2767 2773 2782 2785 2806 2808 2820 2825\n",
      " 2834 2850 2864 2877 2879 2887 2890 2895 2913 2914 2919 2926 2927 2929\n",
      " 2943 2952 2968 2969 2975 2999 3002 3007 3011 3016 3033 3035 3037 3039\n",
      " 3040 3041 3060 3078 3079 3083 3085 3087 3090 3092 3097 3122 3127 3137\n",
      " 3143 3172 3174 3186 3187 3189 3198 3203 3205 3219 3221 3230 3246 3255\n",
      " 3256 3263 3267 3276 3278 3280 3292 3295 3315 3327 3329 3333 3343 3355\n",
      " 3359 3360 3361 3384 3390 3393 3394 3415 3416 3420 3422 3427 3458 3474\n",
      " 3476 3493 3515 3525 3540 3564 3581 3589 3593 3596 3609 3614 3673 3696\n",
      " 3698 3701 3708 3715 3718 3723 3727 3731 3733 3748 3753 3769 3776 3788\n",
      " 3805 3822 3832 3839 3848 3860 3862 3863 3892 3906 3908 3926 3944 3978\n",
      " 3996 4001 4003 4015 4018 4034 4049 4063 4065 4069 4074 4082 4091 4111\n",
      " 4123 4125 4158 4169 4182 4191 4196 4209 4212 4216 4220 4223 4229 4251\n",
      " 4264 4274 4307 4338 4357 4364 4365 4367 4373 4379 4391 4448 4463 4465\n",
      " 4469 4471 4489 4506 4512 4528 4532 4537 4539 4541 4556 4576 4578 4581\n",
      " 4584 4599 4600 4603 4620 4623 4640 4641 4660 4662 4669 4679 4681 4688\n",
      " 4725 4730 4749 4754 4759 4762 4780 4781 4783 4784 4788 4791 4805 4813\n",
      " 4814 4840 4862 4871 4876 4878 4881 4915 4921 4930 4941 4977 4980 4984\n",
      " 4991 4995 5029 5031 5038 5060 5090 5115 5119 5121 5123 5138 5140 5148\n",
      " 5180 5194 5210 5212 5213 5214 5216 5218 5226 5230 5240 5245 5255 5266\n",
      " 5267 5283 5287 5289 5294 5296 5312 5322 5323 5326 5353 5355 5357 5362\n",
      " 5364 5383 5389 5398 5399 5406 5414 5415 5417 5427 5436 5449 5452 5459\n",
      " 5461 5470 5487 5499 5508 5511 5512 5526 5528 5530 5531 5535 5549 5557\n",
      " 5566 5568 5571 5579 5582 5586 5592 5609 5614 5635 5638 5645 5679 5680\n",
      " 5686 5688 5693 5703 5714 5718 5720 5733 5750 5761 5775 5782 5786 5789\n",
      " 5803 5806 5813 5816 5820 5822 5833 5845 5852 5854 5865 5866 5880 5898\n",
      " 5901 5943 5961 5963 5974 5976 5990 6009 6010 6028 6031 6032 6053 6061\n",
      " 6080 6086 6088 6097 6099 6118 6123 6125 6139 6158 6159 6170 6171 6175\n",
      " 6183 6186 6187 6196 6206 6208 6218 6221 6240 6242 6248 6258 6272 6280\n",
      " 6290 6299 6300 6323 6364 6372 6394 6415 6416 6420 6421 6424 6426 6434\n",
      " 6437 6447 6454 6460 6466 6470 6479 6495 6502 6517 6532 6535 6539 6546\n",
      " 6553 6556 6563 6571 6582 6583 6585 6605 6618 6622 6625 6626 6630 6657\n",
      " 6669 6670 6686 6703 6708 6714 6734 6751 6753 6757 6763 6792 6805 6807\n",
      " 6809 6823 6830 6831 6842 6843 6859 6867 6869 6896 6919 6932 6937 6945\n",
      " 6951 6959 6964 6971 6975 6978 6995 6997 7029 7033 7064 7090 7096 7112\n",
      " 7114 7140 7141 7148 7149 7163 7175 7176 7179 7191 7194 7196 7204 7219\n",
      " 7229 7238 7250 7254 7260 7264 7272 7279 7302 7305 7313 7318 7328 7343\n",
      " 7351 7364 7367 7390 7396 7423 7440 7448 7449 7463 7464 7467 7473 7494\n",
      " 7497 7515 7531 7536 7537 7543 7545 7591 7593 7595 7602 7614 7617 7632\n",
      " 7636 7637 7641 7649 7650 7651 7668 7675 7676 7678 7687 7700 7712 7719\n",
      " 7725 7734 7735 7737 7745 7758 7759 7766 7781 7798 7828 7853 7854 7866\n",
      " 7871 7885 7890 7891 7900] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:2853: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    }
   ],
   "source": [
    "n_features = [10, 50, 100, 200, 500, 1000, 2000, 5000, 7000]\n",
    "ensemble_results = pd.DataFrame()\n",
    "for n in n_features:\n",
    "    selectors = [RandomForestSelector(n_features=n), SelectKBest(f_classif, k=n), SelectKBest(chi2, k=n), CorrelationSelector(n_features=n), MutualInformationSelector(n_features=n)]\n",
    "    ensemble = [EnsembleSelector(selectors=selectors)]\n",
    "    df = full_evaluation(train_data, train_labels, valid_data, valid_labels, ensemble, classifiers)\n",
    "    ensemble_results = pd.concat([ensemble_results, df])\n",
    "    \n",
    "ensemble_results.to_csv('data2/ensemble.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
